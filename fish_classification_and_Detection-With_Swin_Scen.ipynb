{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a014650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmengine.registry import DATASETS\n",
    "from mmdet.utils import register_all_modules\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmdet.visualization import DetLocalVisualizer\n",
    "\n",
    "# ‚úÖ Paksa torch.load agar selalu pakai weights_only=False\n",
    "# Ini penting untuk kompatibilitas PyTorch versi baru\n",
    "_orig_torch_load = torch.load\n",
    "def torch_load_wrapper(*args, **kwargs):\n",
    "    kwargs[\"weights_only\"] = False\n",
    "    return _orig_torch_load(*args, **kwargs)\n",
    "\n",
    "torch.load = torch_load_wrapper\n",
    "\n",
    "# ========== KONFIG DASAR ==========\n",
    "DEBUG_MODE = False\n",
    "print(\"=\"*50)\n",
    "print(f\"      MODE DEBUGGING: {'AKTIF' if DEBUG_MODE else 'NONAKTIF'}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "register_all_modules()\n",
    "\n",
    "print(\"--- Tes Ketersediaan GPU PyTorch ---\")\n",
    "print(f\"Apakah CUDA tersedia? -> {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU terdeteksi: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"PERINGATAN: Tidak ada GPU. Training di CPU akan lambat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b99c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 1: Load Config RetinaNet + Swin Backbone =====\n",
    "print(\"--- INFO: Memilih model backbone Swin Transformer ---\")\n",
    "cfg_path = 'mmdetection/configs/swin/retinanet_swin-s-p4-w7_fpn_1x_coco_custom.py'\n",
    "cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "model_name = \"retinanet\" if \"retinanet\" in cfg_path.lower() else \"custommodel\"\n",
    "print(f\"--- INFO: Model yang digunakan: {model_name} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0d5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 2: Definisikan Pipeline Dasar =====\n",
    "print(\"--- INFO: Mendefinisikan pipeline dasar untuk training dan validasi ---\")\n",
    "\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "    dict(type='Resize', scale=(416, 416), keep_ratio=True),\n",
    "    dict(type='Pad', size=(416, 416), pad_val=dict(img=(114, 114, 114))),\n",
    "]\n",
    "\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='Resize', scale=(416, 416), keep_ratio=True),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "    dict(type='PackDetInputs',\n",
    "         meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "]\n",
    "\n",
    "print(\"--- INFO: Definisi pipeline selesai. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312666c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 3: Dataset & Kelas =====\n",
    "CLASSES = (\n",
    "    'Betta', 'Discuss', 'Glofish', 'Goldfish', 'Guppy', 'Gurami',\n",
    "    'Manfish', 'Molly', 'Swordtail', 'Tiger Barb'\n",
    ")\n",
    "cat_id_map = {\n",
    "    1: 0, 3: 1, 4: 2, 5: 3, 6: 4, 7: 5, 9: 6, 10: 7, 11: 8, 12: 9\n",
    "}\n",
    "PALETTE = [\n",
    "    (220, 20, 60), (0, 128, 255), (60, 179, 113), (255, 140, 0), (138, 43, 226),\n",
    "    (255, 215, 0), (199, 21, 133), (127, 255, 212), (70, 130, 180), (255, 99, 71)\n",
    "]\n",
    "metainfo = {'classes': CLASSES, 'palette': PALETTE, 'cat2label': cat_id_map}\n",
    "\n",
    "print(f\"--- INFO: Konfigurasi diatur untuk melatih {len(CLASSES)} kelas. ---\")\n",
    "\n",
    "data_root = 'dataset/datasetcampuran/'\n",
    "train_ann_file = 'train/_annotations.coco.json'\n",
    "val_ann_file   = 'valid/_annotations.coco.json'\n",
    "\n",
    "# --- Konfigurasi Dataset ---\n",
    "train_dataset_cfg = dict(\n",
    "    type='CocoDataset',\n",
    "    data_root=data_root,\n",
    "    metainfo=metainfo,\n",
    "    ann_file=train_ann_file,\n",
    "    data_prefix=dict(img='train/'),\n",
    "    filter_cfg=dict(filter_empty_gt=True),\n",
    "    pipeline=train_pipeline\n",
    ")\n",
    "cfg.train_dataloader.dataset = dict(\n",
    "    type='MultiImageMixDataset',\n",
    "    dataset=train_dataset_cfg,\n",
    "    pipeline=[\n",
    "        dict(type='PhotoMetricDistortion'),\n",
    "        dict(type='RandomFlip', prob=0.5),\n",
    "        dict(type='Normalize',\n",
    "             mean=[123.675, 116.28, 103.53],\n",
    "             std=[58.395, 57.12, 57.375],\n",
    "             to_rgb=True),\n",
    "        dict(type='PackDetInputs')\n",
    "    ]\n",
    ")\n",
    "cfg.val_dataloader.dataset.update(dict(\n",
    "    data_root=data_root, metainfo=metainfo, ann_file=val_ann_file,\n",
    "    data_prefix=dict(img='valid/'), pipeline=val_pipeline\n",
    "))\n",
    "cfg.test_dataloader.dataset.update(dict(\n",
    "    data_root=data_root, metainfo=metainfo, ann_file=val_ann_file,\n",
    "    data_prefix=dict(img='valid/'), pipeline=val_pipeline\n",
    "))\n",
    "\n",
    "cfg.train_dataloader.batch_size = 4\n",
    "cfg.val_dataloader.batch_size = 4\n",
    "cfg.test_dataloader.batch_size = 4\n",
    "cfg.train_dataloader.num_workers = 4\n",
    "cfg.val_dataloader.num_workers = 4\n",
    "cfg.test_dataloader.num_workers = 4\n",
    "print(f\"--- INFO: Batch size: {cfg.train_dataloader.batch_size}, Num workers: {cfg.train_dataloader.num_workers} ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88584e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 4: Evaluator COCO (bbox) + classwise =====\n",
    "cfg.val_evaluator.ann_file = os.path.join(data_root, val_ann_file)\n",
    "cfg.test_evaluator.ann_file = os.path.join(data_root, val_ann_file)\n",
    "cfg.val_evaluator.metric = 'bbox'\n",
    "cfg.test_evaluator.metric = 'bbox'\n",
    "cfg.val_evaluator.classwise = True\n",
    "cfg.test_evaluator.classwise = True\n",
    "cfg.val_evaluator.outfile_prefix = os.path.join('work_eval', 'val_results')\n",
    "cfg.test_evaluator.outfile_prefix = os.path.join('work_eval', 'test_results')\n",
    "os.makedirs('work_eval', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9eb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 5: Atur jumlah kelas di head =====\n",
    "if hasattr(cfg.model, 'bbox_head'):\n",
    "    cfg.model.bbox_head.num_classes = len(CLASSES)\n",
    "elif hasattr(cfg.model, 'roi_head'):\n",
    "    if isinstance(cfg.model.roi_head.bbox_head, list):\n",
    "        for head in cfg.model.roi_head.bbox_head:\n",
    "            head.num_classes = len(CLASSES)\n",
    "    else:\n",
    "        cfg.model.roi_head.bbox_head.num_classes = len(CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffdfd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 6: Training Settings =====\n",
    "cfg.optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    optimizer=dict(type='AdamW', lr=0.00001, weight_decay=0.05),\n",
    "    paramwise_cfg=dict(custom_keys={'norm': dict(decay_mult=0.)})\n",
    ")\n",
    "cfg.param_scheduler = []\n",
    "print(\"--- INFO: Menggunakan Optimizer AdamW dengan LR statis 0.00001 ---\")\n",
    "\n",
    "best_checkpoint_path = './outputs_bs8_lr0001_workers4/epoch_1488.pth'\n",
    "if not DEBUG_MODE and os.path.exists(best_checkpoint_path):\n",
    "    cfg.load_from = best_checkpoint_path\n",
    "    print(f\"--- INFO: Memuat bobot dari: {cfg.load_from} ---\")\n",
    "else:\n",
    "    cfg.load_from = None\n",
    "    print(\"--- INFO: Memulai training dari awal (scratch). ---\")\n",
    "\n",
    "cfg.train_cfg.max_epochs = 200 if not DEBUG_MODE else 2\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "cfg.default_hooks.checkpoint.interval = 1\n",
    "cfg.default_hooks.checkpoint.max_keep_ckpts = 3\n",
    "cfg.default_hooks.checkpoint.save_best = 'coco/bbox_mAP'\n",
    "cfg.default_hooks.checkpoint.rule = 'greater'\n",
    "cfg.visualizer.vis_backends = [\n",
    "    dict(type='LocalVisBackend'),\n",
    "    dict(type='TensorboardVisBackend')\n",
    "]\n",
    "print(f\"--- INFO: Training akan berjalan selama {cfg.train_cfg.max_epochs} epoch. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67a17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 7: Work dir =====\n",
    "work_dir_base = f'./outputs_dataset_campuran'\n",
    "cfg.work_dir = f\"{work_dir_base}_debug\" if DEBUG_MODE else work_dir_base\n",
    "print(f\"--- INFO: Output ke: {cfg.work_dir} ---\")\n",
    "os.makedirs(cfg.work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1517e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 8: Membangun Dataset untuk Pengecekan =====\n",
    "print(\"\\n--- Membangun Dataset ---\")\n",
    "try:\n",
    "    train_dataset = DATASETS.build(cfg.train_dataloader.dataset)\n",
    "    val_dataset = DATASETS.build(cfg.val_dataloader.dataset)\n",
    "    print(f\"Jumlah data training: {len(train_dataset)}\")\n",
    "    print(f\"Jumlah data validasi: {len(val_dataset)}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nFATAL ERROR memuat dataset: {e}\")\n",
    "    raise e\n",
    "print(\"---------------------------------\\n\")\n",
    "\n",
    "print(\"Konfigurasi selesai. Siap training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb04033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 9: Training =====\n",
    "print(\">>> Mulai training...\")\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()\n",
    "print(\">>> Training selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889bec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================\n",
    "# BAGIAN BARU: ANALISIS DAN PLOTTING OTOMATIS SETELAH TRAINING \n",
    "# =================================================================================\n",
    "\n",
    "# --- Direktori Laporan ---\n",
    "report_dir = os.path.join(cfg.work_dir, 'reports_epoch30_batchsize8_lr0.0001(2)')\n",
    "os.makedirs(report_dir, exist_ok=True)\n",
    "print(f\"\\n--- Memulai Analisis Pasca-Training. Laporan akan disimpan di: {report_dir} ---\")\n",
    "\n",
    "\n",
    "# ========== LANGKAH 1: EVALUASI STANDAR MMDETECTION (UNTUK coco_eval_summary.txt) ==========\n",
    "print(\"\\n>>> Menjalankan evaluasi COCO standar...\")\n",
    "eval_results = runner.test() \n",
    "\n",
    "# Simpan ringkasan evaluasi standar ke file teks\n",
    "eval_summary_path = os.path.join(report_dir, 'coco_eval_summary.txt')\n",
    "with open(eval_summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=== COCO BBox Evaluation (mAP) ===\\n\")\n",
    "    for k, v in eval_results.items():\n",
    "        # Filter untuk hanya menulis metrik yang relevan (mAP dan presisi per kelas)\n",
    "        if 'mAP' in k or 'precision' in k:\n",
    "            f.write(f\"{k}: {v:.4f}\\n\")\n",
    "print(f\">>> Ringkasan evaluasi COCO disimpan di: {eval_summary_path}\")\n",
    "\n",
    "# ========== FUNGSI UNTUK PLOTTING KURVA TRAINING ==========\n",
    "# ========== FUNGSI UNTUK PLOTTING KURVA TRAINING (VERSI FINAL YANG BENAR) ==========\n",
    "def create_stability_curve_by_epoch(work_dir, output_dir):\n",
    "    print(\"\\n>>> Membuat grafik stabilitas training (Loss vs. mAP)...\")\n",
    "    \n",
    "    # Bagian pencarian file ini sudah benar dan akan bekerja secara otomatis\n",
    "    log_file = './outputs_dataset_campuran/20251119_140329/vis_data/20251119_140329.json'\n",
    "    vis_data_path = os.path.join(work_dir, 'vis_data')\n",
    "    if os.path.exists(vis_data_path):\n",
    "        # Cari file .json, tapi abaikan scalars.json\n",
    "        log_files = [f for f in os.listdir(vis_data_path) if f.endswith('.json') and 'scalars' not in f]\n",
    "        if log_files:\n",
    "            # Urutkan berdasarkan waktu modifikasi untuk mendapatkan yang terbaru\n",
    "            log_files.sort(key=lambda x: os.path.getmtime(os.path.join(vis_data_path, x)))\n",
    "            log_file = os.path.join(vis_data_path, log_files[-1])\n",
    "\n",
    "    if not log_file or not os.path.exists(log_file):\n",
    "        print(f\"--- PERINGATAN: File log .json tidak ditemukan di dalam '{vis_data_path}'. Grafik stabilitas tidak dapat dibuat. ---\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- INFO: Membaca log dari: {log_file} ---\")\n",
    "    \n",
    "    # --- LOGIKA PARSING BARU DAN BENAR ---\n",
    "    epoch_data = {}\n",
    "    current_epoch = 0 # Variabel untuk melacak epoch terakhir yang terlihat\n",
    "\n",
    "    with open(log_file, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                log = json.loads(line.strip())\n",
    "                \n",
    "                # KASUS 1: Ini adalah log training (karena memiliki 'loss' dan 'epoch')\n",
    "                if 'loss' in log and 'epoch' in log:\n",
    "                    epoch = log['epoch']\n",
    "                    current_epoch = epoch # Perbarui epoch saat ini\n",
    "                    \n",
    "                    # Buat entri untuk epoch jika ini yang pertama kali\n",
    "                    if epoch not in epoch_data:\n",
    "                        epoch_data[epoch] = {'train_loss': [], 'val_map': 0}\n",
    "                    \n",
    "                    epoch_data[epoch]['train_loss'].append(log['loss'])\n",
    "\n",
    "                # KASUS 2: Ini adalah log validasi (karena memiliki 'coco/bbox_mAP')\n",
    "                elif 'coco/bbox_mAP' in log:\n",
    "                    # Pastikan kita sudah melihat setidaknya satu epoch training\n",
    "                    if current_epoch > 0 and current_epoch in epoch_data:\n",
    "                        epoch_data[current_epoch]['val_map'] = log['coco/bbox_mAP']\n",
    "\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                # Abaikan baris yang bukan JSON valid atau tidak memiliki kunci yang diharapkan\n",
    "                continue\n",
    "\n",
    "    # Bagian agregasi dan plotting tidak berubah dan sekarang akan berfungsi\n",
    "    epochs, avg_losses, val_maps = [], [], []\n",
    "    for epoch, data in sorted(epoch_data.items()):\n",
    "        if data['train_loss'] and data['val_map'] > 0:\n",
    "            epochs.append(epoch)\n",
    "            avg_losses.append(np.mean(data['train_loss']))\n",
    "            val_maps.append(data['val_map'])\n",
    "\n",
    "    if not epochs:\n",
    "        print(\"--- GAGAL: Tidak dapat mengekstrak pasangan data (loss & mAP) yang valid dari log. Periksa isi file log. ---\")\n",
    "        return\n",
    "\n",
    "    # Sisa dari fungsi plotting...\n",
    "    fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Rata-rata Training Loss', color=color)\n",
    "    ax1.plot(epochs, avg_losses, 'o-', color=color, label='Training Loss (rata-rata)')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Validation mAP', color=color)\n",
    "    ax2.plot(epochs, val_maps, 's-', color=color, label='Validation mAP')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "    plt.title('Kurva Stabilitas Training: Loss vs. Validation mAP')\n",
    "    fig.tight_layout()\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='best')\n",
    "\n",
    "    plot_path = os.path.join(output_dir, 'training_stability_curve.png')\n",
    "    plt.savefig(plot_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\">>> Grafik stabilitas training berhasil disimpan di: {plot_path}\")\n",
    "\n",
    "\n",
    "# ========== FUNGSI UNTUK CONFUSION MATRIX & PR CURVE ==========\n",
    "def compute_iou_matrix(boxes1, boxes2):\n",
    "    if boxes1.size == 0 or boxes2.size == 0: return np.zeros((boxes1.shape[0], boxes2.shape[0]), dtype=np.float32)\n",
    "    x11, y11, x12, y12 = boxes1[:,0], boxes1[:,1], boxes1[:,2], boxes1[:,3]\n",
    "    x21, y21, x22, y22 = boxes2[:,0], boxes2[:,1], boxes2[:,2], boxes2[:,3]\n",
    "    inter_x1 = np.maximum(x11[:, None], x21[None, :]); inter_y1 = np.maximum(y11[:, None], y21[None, :])\n",
    "    inter_x2 = np.minimum(x12[:, None], x22[None, :]); inter_y2 = np.minimum(y12[:, None], y22[None, :])\n",
    "    inter_w = np.maximum(0, inter_x2 - inter_x1); inter_h = np.maximum(0, inter_y2 - inter_y1)\n",
    "    inter = inter_w * inter_h\n",
    "    area1 = (x12 - x11) * (y12 - y11); area2 = (x22 - x21) * (y22 - y21)\n",
    "    union = area1[:, None] + area2[None, :] - inter\n",
    "    return np.where(union > 0, inter / union, 0.0)\n",
    "\n",
    "def greedy_match(iou_mat, iou_thr=0.5):\n",
    "    matches = []; gt_used, pred_used = set(), set()\n",
    "    if iou_mat.size == 0: return matches\n",
    "    pairs = sorted([(i, j, iou_mat[i, j]) for i in range(iou_mat.shape[0]) for j in range(iou_mat.shape[1])], key=lambda x: x[2], reverse=True)\n",
    "    for i, j, iou in pairs:\n",
    "        if iou < iou_thr: break\n",
    "        if i in gt_used or j in pred_used: continue\n",
    "        gt_used.add(i); pred_used.add(j); matches.append((i, j))\n",
    "    return matches\n",
    "def evaluate_confusion_pr(model, dataset, output_dir, score_thr=0.3, iou_thr=0.5):\n",
    "    print(\"\\n>>> Memulai evaluasi kustom (Confusion Matrix, PR Curves & Metrik Per Kelas)...\")\n",
    "    base_dataset = dataset.dataset if hasattr(dataset, 'dataset') else dataset\n",
    "    n_classes = len(CLASSES)\n",
    "    conf_mat = np.zeros((n_classes, n_classes), dtype=np.int32)\n",
    "    per_class_counts = {'TP': np.zeros(n_classes, dtype=np.int32), 'FP': np.zeros(n_classes, dtype=np.int32), 'FN': np.zeros(n_classes, dtype=np.int32)}\n",
    "    pr_store = {c: {'scores': [], 'match': []} for c in range(n_classes)}\n",
    "\n",
    "    for idx in range(len(base_dataset)):\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"  Mengevaluasi gambar {idx+1}/{len(base_dataset)}...\")\n",
    "        data_info = base_dataset.get_data_info(idx)\n",
    "        img_path = data_info['img_path']\n",
    "        gt_instances = data_info.get('instances', [])\n",
    "        if gt_instances:\n",
    "            gt_bboxes = np.array([inst['bbox'] for inst in gt_instances], dtype=np.float32)\n",
    "            gt_labels = np.array([inst['bbox_label'] for inst in gt_instances], dtype=np.int64)\n",
    "        else:\n",
    "            gt_bboxes = np.empty((0, 4), dtype=np.float32)\n",
    "            gt_labels = np.empty((0,), dtype=np.int64)\n",
    "        \n",
    "        result = inference_detector(model, img_path)\n",
    "        pred = result.pred_instances\n",
    "        keep = pred.scores >= score_thr\n",
    "        pred_bboxes, pred_scores, pred_labels = pred.bboxes[keep].cpu().numpy(), pred.scores[keep].cpu().numpy(), pred.labels[keep].cpu().numpy()\n",
    "\n",
    "        iou_mat = compute_iou_matrix(gt_bboxes, pred_bboxes)\n",
    "        matches = greedy_match(iou_mat, iou_thr=iou_thr)\n",
    "        matched_gt, matched_pred = {m[0] for m in matches}, {m[1] for m in matches}\n",
    "\n",
    "        for gi, pj in matches:\n",
    "            gt_c, pd_c = int(gt_labels[gi]), int(pred_labels[pj])\n",
    "            if gt_c == pd_c:\n",
    "                per_class_counts['TP'][pd_c] += 1\n",
    "                pr_store[pd_c]['scores'].append(float(pred_scores[pj]))\n",
    "                pr_store[pd_c]['match'].append(1)\n",
    "            conf_mat[gt_c, pd_c] += 1\n",
    "\n",
    "        for j, pd_c_int in enumerate(pred_labels.astype(int)):\n",
    "            if j not in matched_pred:\n",
    "                per_class_counts['FP'][pd_c_int] += 1\n",
    "                pr_store[pd_c_int]['scores'].append(float(pred_scores[j]))\n",
    "                pr_store[pd_c_int]['match'].append(0)\n",
    "\n",
    "        for i, gt_c_int in enumerate(gt_labels.astype(int)):\n",
    "            if i not in matched_gt:\n",
    "                per_class_counts['FN'][gt_c_int] += 1\n",
    "\n",
    "    # --- BAGIAN YANG DIKEMBALIKAN: Perhitungan dan Penulisan File per_class_metrics.txt ---\n",
    "    eps = 1e-12\n",
    "    prec = per_class_counts['TP'] / (per_class_counts['TP'] + per_class_counts['FP'] + eps)\n",
    "    rec  = per_class_counts['TP'] / (per_class_counts['TP'] + per_class_counts['FN'] + eps)\n",
    "    f1   = 2 * prec * rec / (prec + rec + eps)\n",
    "\n",
    "    lines = [\"KELAS,TP,FP,FN,Precision,Recall,F1\"]\n",
    "    for c, name in enumerate(CLASSES):\n",
    "        lines.append(f\"{name},{per_class_counts['TP'][c]},{per_class_counts['FP'][c]},{per_class_counts['FN'][c]},\"\n",
    "                     f\"{prec[c]:.4f},{rec[c]:.4f},{f1[c]:.4f}\")\n",
    "\n",
    "    per_class_txt_path = os.path.join(output_dir, \"per_class_metrics.txt\")\n",
    "    with open(per_class_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "    print(f\">>> Metrik per kelas disimpan di: {per_class_txt_path}\")\n",
    "    # --- AKHIR DARI BAGIAN YANG DIKEMBALIKAN ---\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(12, 10)); ax = fig_cm.add_subplot(111)\n",
    "    im = ax.imshow(conf_mat, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_title(f\"Confusion Matrix (IoU>{iou_thr}, Score>{score_thr})\"); plt.colorbar(im, ax=ax)\n",
    "    tick_marks = np.arange(len(CLASSES)); ax.set_xticks(tick_marks, labels=CLASSES, rotation=45, ha='right'); ax.set_yticks(tick_marks, labels=CLASSES)\n",
    "    thresh = conf_mat.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "        ax.text(j, i, format(conf_mat[i, j], 'd'), ha=\"center\", va=\"center\", color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True Label'); plt.xlabel('Predicted Label'); plt.tight_layout()\n",
    "    cm_path = os.path.join(output_dir, \"confusion_matrix.png\"); fig_cm.savefig(cm_path, dpi=200); plt.close(fig_cm)\n",
    "    print(f\">>> Confusion matrix disimpan di: {cm_path}\")\n",
    "\n",
    "    # Plot PR Curves\n",
    "    for c in range(len(CLASSES)):\n",
    "        # ... (sisa kode plotting PR curve tetap sama)\n",
    "        scores, match = np.array(pr_store[c]['scores']), np.array(pr_store[c]['match'])\n",
    "        if scores.size == 0: continue\n",
    "        order = np.argsort(-scores); scores, match = scores[order], match[order]\n",
    "        tp_cum, fp_cum = np.cumsum(match), np.cumsum(1 - match)\n",
    "        precision = tp_cum / np.maximum(tp_cum + fp_cum, 1)\n",
    "        total_pos = per_class_counts['TP'][c] + per_class_counts['FN'][c]\n",
    "        recall = tp_cum / max(total_pos, 1) if total_pos > 0 else np.zeros_like(tp_cum)\n",
    "        fig_pr = plt.figure(); plt.plot(recall, precision, '-o', markersize=4); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR Curve - {CLASSES[c]}\"); plt.grid(); plt.xlim(-0.05, 1.05); plt.ylim(-0.05, 1.05)\n",
    "        pr_path = os.path.join(output_dir, f\"pr_curve_{c:02d}_{CLASSES[c].replace(' ','_')}.png\"); fig_pr.savefig(pr_path, dpi=200, bbox_inches='tight'); plt.close(fig_pr)\n",
    "    print(f\">>> PR curves disimpan di: {output_dir}\")\n",
    "\n",
    "create_stability_curve_by_epoch(runner.work_dir, report_dir)\n",
    "\n",
    "# --- B. Alur Eksekusi untuk Evaluasi Kustom ---\n",
    "# Path ke checkpoint sekarang juga didasarkan pada runner.work_dir\n",
    "best_checkpoint_file = None\n",
    "best_ckpts = [f for f in os.listdir(runner.work_dir) if f.startswith('best_') and f.endswith('.pth')]\n",
    "if best_ckpts:\n",
    "    best_ckpts.sort()\n",
    "    best_checkpoint_file = os.path.join(runner.work_dir, best_ckpts[-1])\n",
    "\n",
    "if best_checkpoint_file and os.path.exists(best_checkpoint_file):\n",
    "    print(f\"\\n--- INFO: Memuat checkpoint terbaik untuk evaluasi kustom: {best_checkpoint_file} ---\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model = init_detector(cfg, best_checkpoint_file, device=device)\n",
    "    \n",
    "    # Jalankan evaluasi kustom menggunakan fungsi yang sudah diperbarui\n",
    "    evaluate_confusion_pr(model, val_dataset, report_dir, score_thr=0.3, iou_thr=0.5)\n",
    "else:\n",
    "    print(f\"--- PERINGATAN: Checkpoint terbaik tidak ditemukan di '{runner.work_dir}'. Evaluasi kustom dilewati. ---\")\n",
    "\n",
    "print(\"\\n--- Semua proses (training dan analisis) telah selesai. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96da2bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ========== STEP 11: INFERENCE + RINGKASAN JUMLAH PER JENIS ==========\n",
    "\n",
    "# --- Setup Direktori dan Visualizer ---\n",
    "vis_save_dir = os.path.join(cfg.work_dir, 'inference_results')\n",
    "os.makedirs(vis_save_dir, exist_ok=True)\n",
    "\n",
    "visualizer = DetLocalVisualizer(\n",
    "    vis_backends=[dict(type='LocalVisBackend')],\n",
    "    name='visualizer',\n",
    "    save_dir=vis_save_dir\n",
    ")\n",
    "\n",
    "# ========== PERBAIKAN UTAMA DI SINI ==========\n",
    "# Ambil metainfo langsung dari model yang sudah diinisialisasi.\n",
    "# Ini adalah cara yang paling aman dan direkomendasikan.\n",
    "visualizer.dataset_meta = model.dataset_meta\n",
    "# ============================================\n",
    "\n",
    "# --- Proses Inferensi pada Gambar Uji ---\n",
    "# Ganti dengan path gambar yang ingin Anda uji\n",
    "img_path = 'dataset/10_jenis_ikan/test/Discuss_149_jpg.rf.7eae5bed233fd271fbdf51d38365de39.jpg'\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"ERROR: Gambar uji tidak ditemukan di '{img_path}'. Melewati langkah inferensi.\")\n",
    "else:\n",
    "    print(f\">>> Melakukan inferensi pada gambar: {os.path.basename(img_path)}\")\n",
    "    result = inference_detector(model, img_path)\n",
    "    pred_instances = result.pred_instances\n",
    "    scores = pred_instances.scores.cpu().numpy()\n",
    "    labels = pred_instances.labels.cpu().numpy()\n",
    "\n",
    "    # Tentukan score threshold untuk memfilter deteksi\n",
    "    score_thr = 0.3\n",
    "    keep_indices = scores >= score_thr\n",
    "    \n",
    "    # Hitung jumlah ikan yang terdeteksi per kelas\n",
    "    labels_kept = labels[keep_indices]\n",
    "    class_names = visualizer.dataset_meta['classes'] # Gunakan kelas dari visualizer\n",
    "    counts = {name: int((labels_kept == i).sum()) for i, name in enumerate(class_names)}\n",
    "    total_detected = int(keep_indices.sum())\n",
    "\n",
    "    # --- Tampilkan dan Simpan Ringkasan Deteksi ---\n",
    "    print(\"\\n>>> Ringkasan Deteksi (Score Threshold > \" + str(score_thr) + \"):\")\n",
    "    for class_name, count in counts.items():\n",
    "        if count > 0:\n",
    "            print(f\"- {class_name}: {count}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Total Ikan Terdeteksi: {total_detected}\")\n",
    "\n",
    "    # Simpan ringkasan ke file teks\n",
    "    summary_txt_path = os.path.join(vis_save_dir, \"detection_summary.txt\")\n",
    "    with open(summary_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Hasil Deteksi pada: {os.path.basename(img_path)}\\n\")\n",
    "        f.write(f\"Score Threshold: {score_thr}\\n\")\n",
    "        f.write(\"=\"*30 + \"\\n\")\n",
    "        for class_name, count in counts.items():\n",
    "             if count > 0:\n",
    "                f.write(f\"{class_name}: {count}\\n\")\n",
    "        f.write(\"=\"*30 + \"\\n\")\n",
    "        f.write(f\"TOTAL TERDETEKSI: {total_detected}\\n\")\n",
    "    print(f\">>> Ringkasan deteksi disimpan di: {summary_txt_path}\")\n",
    "\n",
    "    # --- Visualisasi Hasil Deteksi dan Simpan ke File ---\n",
    "    img = mmcv.imread(img_path)\n",
    "    img = mmcv.imconvert(img, 'bgr', 'rgb') # Konversi BGR (OpenCV) ke RGB (Matplotlib)\n",
    "    \n",
    "    output_image_path = os.path.join(vis_save_dir, f\"result_{os.path.basename(img_path)}\")\n",
    "    \n",
    "    visualizer.add_datasample(\n",
    "        name='prediction',\n",
    "        image=img,\n",
    "        data_sample=result,\n",
    "        draw_gt=False,\n",
    "        show=False,           # Set True jika Anda ingin gambar muncul di jendela popup\n",
    "        wait_time=0,\n",
    "        pred_score_thr=score_thr,\n",
    "        out_file=output_image_path\n",
    "    )\n",
    "    print(f\">>> Gambar hasil deteksi disimpan di: {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6262397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Patch PyTorch 2.6+ diterapkan: weights_only=False diaktifkan.\n",
      "=== MULAI ANALISIS (INPUT MANUAL) ===\n",
      "\n",
      "[1/5] Memproses: Model 5\n",
      "      üìã Copy Berhasil: 20251024_013428.json\n",
      "      üìã Copy Berhasil: best_coco_bbox_mAP_epoch_140.pth\n",
      "      üìä Membuat grafik stabilitas...\n",
      "      üöÄ Memuat model untuk Confusion Matrix...\n",
      "Loads checkpoint by local backend from path: outputs_bs8_lr0001_workers4/best_coco_bbox_mAP_epoch_140.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "      üì∏ Evaluasi 484 gambar...\n",
      "         Processing 0/484...\n",
      "         Processing 100/484...\n",
      "         Processing 200/484...\n",
      "         Processing 300/484...\n",
      "         Processing 400/484...\n",
      "      ‚úÖ CM Selesai.\n",
      "\n",
      "[2/5] Memproses: Model 8\n",
      "      üìã Copy Berhasil: 20251023_112231.json\n",
      "      üìã Copy Berhasil: best_coco_bbox_mAP_epoch_120.pth\n",
      "      üìä Membuat grafik stabilitas...\n",
      "      üöÄ Memuat model untuk Confusion Matrix...\n",
      "Loads checkpoint by local backend from path: outputs_bs8_lr0001_workers4/best_coco_bbox_mAP_epoch_120.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "      üì∏ Evaluasi 484 gambar...\n",
      "         Processing 0/484...\n",
      "         Processing 100/484...\n",
      "         Processing 200/484...\n",
      "         Processing 300/484...\n",
      "         Processing 400/484...\n",
      "      ‚úÖ CM Selesai.\n",
      "\n",
      "[3/5] Memproses: Model 9\n",
      "      üìã Copy Berhasil: 20251029_101043.json\n",
      "      üìã Copy Berhasil: best_coco_bbox_mAP_epoch_27.pth\n",
      "      üìä Membuat grafik stabilitas...\n",
      "      üöÄ Memuat model untuk Confusion Matrix...\n",
      "Loads checkpoint by local backend from path: outputs_model_dengan_grafik/best_coco_bbox_mAP_epoch_27.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "      üì∏ Evaluasi 484 gambar...\n",
      "         Processing 0/484...\n",
      "         Processing 100/484...\n",
      "         Processing 200/484...\n",
      "         Processing 300/484...\n",
      "         Processing 400/484...\n",
      "      ‚úÖ CM Selesai.\n",
      "\n",
      "[4/5] Memproses: Model 10\n",
      "      üìã Copy Berhasil: 20251119_064040.json\n",
      "      üìã Copy Berhasil: best_coco_bbox_mAP_epoch_191.pth\n",
      "      üìä Membuat grafik stabilitas...\n",
      "      üöÄ Memuat model untuk Confusion Matrix...\n",
      "Loads checkpoint by local backend from path: outputs_dataset_pure/best_coco_bbox_mAP_epoch_191.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "      üì∏ Evaluasi 484 gambar...\n",
      "         Processing 0/484...\n",
      "         Processing 100/484...\n",
      "         Processing 200/484...\n",
      "         Processing 300/484...\n",
      "         Processing 400/484...\n",
      "      ‚úÖ CM Selesai.\n",
      "\n",
      "[5/5] Memproses: Model 11\n",
      "      üìã Copy Berhasil: 20251119_140329.json\n",
      "      üìã Copy Berhasil: best_coco_bbox_mAP_epoch_186.pth\n",
      "      üìä Membuat grafik stabilitas...\n",
      "      üöÄ Memuat model untuk Confusion Matrix...\n",
      "Loads checkpoint by local backend from path: outputs_dataset_campuran/best_coco_bbox_mAP_epoch_186.pth\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "      üì∏ Evaluasi 484 gambar...\n",
      "         Processing 0/484...\n",
      "         Processing 100/484...\n",
      "         Processing 200/484...\n",
      "         Processing 300/484...\n",
      "         Processing 400/484...\n",
      "      ‚úÖ CM Selesai.\n",
      "\n",
      "üéâ SEMUA PROSES SELESAI!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import shutil\n",
    "from collections import defaultdict\n",
    "from mmengine.config import Config\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "from mmengine.registry import DATASETS\n",
    "from mmdet.utils import register_all_modules\n",
    "\n",
    "# ===================================================================\n",
    "# FIX UNTUK PYTORCH 2.6+ (UNPICKLING ERROR)\n",
    "# ===================================================================\n",
    "_original_torch_load = torch.load\n",
    "def _safe_torch_load(*args, **kwargs):\n",
    "    if 'weights_only' not in kwargs:\n",
    "        kwargs['weights_only'] = False\n",
    "    return _original_torch_load(*args, **kwargs)\n",
    "torch.load = _safe_torch_load\n",
    "print(\"‚úÖ Patch PyTorch 2.6+ diterapkan: weights_only=False diaktifkan.\")\n",
    "\n",
    "# ===================================================================\n",
    "# BAGIAN 1: KONFIGURASI MANUAL (EDIT DI SINI)\n",
    "# ===================================================================\n",
    "\n",
    "# Masukkan path LENGKAP ke file .json dan .pth untuk setiap model.\n",
    "# Jika file tidak ada, biarkan string kosong '' atau None.\n",
    "\n",
    "DAFTAR_MODEL = [\n",
    "    {\n",
    "        'nama': 'Model 5',\n",
    "        'log_path': 'outputs_bs8_lr0001_workers4/20251024_013428/vis_data/20251024_013428.json',\n",
    "        'ckpt_path': 'outputs_bs8_lr0001_workers4/best_coco_bbox_mAP_epoch_140.pth', # <--- Ganti dengan nama file pth yg benar\n",
    "        'output_folder': '5_model_terpilih/Model 5' \n",
    "    },\n",
    "    {\n",
    "        'nama': 'Model 8',\n",
    "        'log_path': 'outputs_bs8_lr0001_workers4/20251023_112231/vis_data/20251023_112231.json',\n",
    "        'ckpt_path': 'outputs_bs8_lr0001_workers4/best_coco_bbox_mAP_epoch_120.pth',\n",
    "        'output_folder': '5_model_terpilih/Model 8' \n",
    "    },\n",
    "    {\n",
    "        'nama': 'Model 9',\n",
    "        'log_path': 'outputs_model_dengan_grafik/20251029_101043/vis_data/20251029_101043.json',\n",
    "        'ckpt_path': 'outputs_model_dengan_grafik/best_coco_bbox_mAP_epoch_27.pth',\n",
    "        'output_folder': '5_model_terpilih/Model 9' \n",
    "    },\n",
    "    {\n",
    "        'nama': 'Model 10',\n",
    "        'log_path': 'outputs_dataset_pure/20251119_064040/vis_data/20251119_064040.json',\n",
    "        'ckpt_path': 'outputs_dataset_pure/best_coco_bbox_mAP_epoch_191.pth', # Contoh nama file lain\n",
    "        'output_folder': '5_model_terpilih/Model 10' \n",
    "    },\n",
    "    {\n",
    "        'nama': 'Model 11',\n",
    "        'log_path': 'outputs_dataset_campuran/20251119_140329/vis_data/20251119_140329.json',\n",
    "        'ckpt_path': 'outputs_dataset_campuran/best_coco_bbox_mAP_epoch_186.pth',\n",
    "        'output_folder': '5_model_terpilih/Model 11' \n",
    "    },\n",
    "]\n",
    "# --- KONFIGURASI DATASET ---\n",
    "CLASSES = (\n",
    "    'Betta', 'Discuss', 'Glofish', 'Goldfish', 'Guppy', 'Gurami',\n",
    "    'Manfish', 'Molly', 'Swordtail', 'Tiger Barb'\n",
    ")\n",
    "BASE_CONFIG_PATH = 'mmdetection/configs/swin/retinanet_swin-s-p4-w7_fpn_1x_coco_custom.py'\n",
    "DATA_ROOT = 'dataset/10_jenis_ikan/'\n",
    "ANN_FILE = 'valid/_annotations.coco.json'\n",
    "IMG_PREFIX = 'valid/'\n",
    "\n",
    "# ===================================================================\n",
    "# BAGIAN 2: FUNGSI UTILITIES\n",
    "# ===================================================================\n",
    "\n",
    "def arsipkan_file(src_path, dst_folder):\n",
    "    \"\"\"Copy file dari src ke dst jika src ada.\"\"\"\n",
    "    if src_path and os.path.exists(src_path):\n",
    "        nama_file = os.path.basename(src_path)\n",
    "        dst_path = os.path.join(dst_folder, nama_file)\n",
    "        try:\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "            print(f\"      üìã Copy Berhasil: {nama_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"      ‚ö†Ô∏è Gagal copy {nama_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"      ‚ùå Gagal Copy: File sumber tidak ditemukan -> {src_path}\")\n",
    "\n",
    "def create_stability_curve(log_path, output_dir, model_name):\n",
    "    print(\"      üìä Membuat grafik stabilitas...\")\n",
    "    epoch_data = defaultdict(lambda: {'losses': [], 'mAP': None})\n",
    "    last_seen_epoch = 0\n",
    "    try:\n",
    "        with open(log_path, 'r') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    log = json.loads(line.strip())\n",
    "                    if 'loss' in log and 'epoch' in log:\n",
    "                        epoch = log['epoch']; epoch_data[epoch]['losses'].append(log['loss']); last_seen_epoch = epoch\n",
    "                    elif 'coco/bbox_mAP' in log:\n",
    "                        if last_seen_epoch > 0: epoch_data[last_seen_epoch]['mAP'] = log['coco/bbox_mAP']\n",
    "                except: continue\n",
    "        \n",
    "        epochs, losses, maps = [], [], []\n",
    "        for ep, data in sorted(epoch_data.items()):\n",
    "            if data['losses'] and data['mAP'] is not None:\n",
    "                epochs.append(ep); losses.append(sum(data['losses'])/len(data['losses'])); maps.append(data['mAP'])\n",
    "\n",
    "        if not epochs: \n",
    "            print(\"      ‚ö†Ô∏è Data log kosong atau tidak valid.\")\n",
    "            return\n",
    "\n",
    "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "        ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss', color='tab:red')\n",
    "        ax1.plot(epochs, losses, 'o-', color='tab:red', label='Loss')\n",
    "        ax2 = ax1.twinx(); ax2.set_ylabel('mAP', color='tab:blue')\n",
    "        ax2.plot(epochs, maps, 's-', color='tab:blue', label='mAP')\n",
    "        plt.title(f'Training Stability: {model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, f'grafik_stabilitas_{model_name}.png'))\n",
    "        plt.close(fig)\n",
    "    except Exception as e: print(f\"      ‚ö†Ô∏è Error Plotting: {e}\")\n",
    "\n",
    "def evaluate_model(ckpt_path, output_dir, model_name):\n",
    "    print(f\"      üöÄ Memuat model untuk Confusion Matrix...\")\n",
    "    \n",
    "    # Init Model\n",
    "    cfg = Config.fromfile(BASE_CONFIG_PATH)\n",
    "    if hasattr(cfg.model, 'bbox_head'): cfg.model.bbox_head.num_classes = len(CLASSES)\n",
    "    elif hasattr(cfg.model, 'roi_head'): \n",
    "        if isinstance(cfg.model.roi_head.bbox_head, list):\n",
    "            for h in cfg.model.roi_head.bbox_head: h.num_classes = len(CLASSES)\n",
    "        else: cfg.model.roi_head.bbox_head.num_classes = len(CLASSES)\n",
    "    \n",
    "    register_all_modules(init_default_scope=False)\n",
    "    \n",
    "    try:\n",
    "        model = init_detector(cfg, ckpt_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    except Exception as e:\n",
    "        print(f\"      ‚ùå Gagal memuat checkpoint: {e}\")\n",
    "        return\n",
    "\n",
    "    # Init Dataset\n",
    "    val_pipeline = [\n",
    "        dict(type='LoadImageFromFile'), dict(type='Resize', scale=(416, 416), keep_ratio=True),\n",
    "        dict(type='LoadAnnotations', with_bbox=True),\n",
    "        dict(type='PackDetInputs', meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "    ]\n",
    "    metainfo = {'classes': CLASSES, 'palette': [(220, 20, 60)] * len(CLASSES)}\n",
    "    \n",
    "    # Membangun Dataset\n",
    "    dataset = DATASETS.build(dict(\n",
    "        type='CocoDataset', data_root=DATA_ROOT, metainfo=metainfo,\n",
    "        ann_file=ANN_FILE, data_prefix=dict(img=IMG_PREFIX),\n",
    "        test_mode=True, pipeline=val_pipeline, filter_cfg=dict(filter_empty_gt=True)))\n",
    "    \n",
    "    print(f\"      üì∏ Evaluasi {len(dataset)} gambar...\")\n",
    "    conf_mat = np.zeros((len(CLASSES), len(CLASSES)), dtype=np.int32)\n",
    "    \n",
    "    # --- PERBAIKAN DI SINI ---\n",
    "    # Gunakan 'dataset' langsung, jangan panggil .dataset lagi\n",
    "    base_ds = dataset \n",
    "    \n",
    "    for idx in range(len(base_ds)):\n",
    "        if idx % 100 == 0: print(f\"         Processing {idx}/{len(base_ds)}...\")\n",
    "        \n",
    "        # get_data_info tersedia langsung di base_ds\n",
    "        info = base_ds.get_data_info(idx)\n",
    "        result = inference_detector(model, info['img_path'])\n",
    "        \n",
    "        gt_labels = np.array([ann['bbox_label'] for ann in info['instances']])\n",
    "        gt_bboxes = np.array([ann['bbox'] for ann in info['instances']]).reshape(-1, 4)\n",
    "        pred = result.pred_instances\n",
    "        keep = pred.scores > 0.3\n",
    "        pred_labels = pred.labels[keep].cpu().numpy()\n",
    "        pred_bboxes = pred.bboxes[keep].cpu().numpy()\n",
    "\n",
    "        if len(gt_bboxes) == 0 or len(pred_bboxes) == 0: continue\n",
    "        \n",
    "        ix1 = np.maximum(gt_bboxes[:, 0:1], pred_bboxes[None, :, 0])\n",
    "        iy1 = np.maximum(gt_bboxes[:, 1:2], pred_bboxes[None, :, 1])\n",
    "        ix2 = np.minimum(gt_bboxes[:, 2:3], pred_bboxes[None, :, 2])\n",
    "        iy2 = np.minimum(gt_bboxes[:, 3:4], pred_bboxes[None, :, 3])\n",
    "        inter = np.maximum(0, ix2 - ix1) * np.maximum(0, iy2 - iy1)\n",
    "        union = ((gt_bboxes[:, 2]-gt_bboxes[:, 0])*(gt_bboxes[:, 3]-gt_bboxes[:, 1]))[:, None] + \\\n",
    "                ((pred_bboxes[:, 2]-pred_bboxes[:, 0])*(pred_bboxes[:, 3]-pred_bboxes[:, 1]))[None, :] - inter\n",
    "        ious = inter / (union + 1e-6)\n",
    "        \n",
    "        gt_matched = set(); pred_matched = set()\n",
    "        for i in range(len(gt_bboxes)):\n",
    "            best_iou = 0; best_j = -1\n",
    "            for j in range(len(pred_bboxes)):\n",
    "                if j in pred_matched: continue\n",
    "                if ious[i, j] > best_iou: best_iou = ious[i, j]; best_j = j\n",
    "            if best_iou >= 0.5:\n",
    "                conf_mat[gt_labels[i], pred_labels[best_j]] += 1\n",
    "                gt_matched.add(i); pred_matched.add(best_j)\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(conf_mat, interpolation='nearest', cmap='Blues')\n",
    "    ax.set_title(f\"Confusion Matrix: {model_name}\")\n",
    "    plt.colorbar(im)\n",
    "    tick_marks = np.arange(len(CLASSES))\n",
    "    ax.set_xticks(tick_marks, CLASSES, rotation=45, ha='right')\n",
    "    ax.set_yticks(tick_marks, CLASSES)\n",
    "    thresh = conf_mat.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "        ax.text(j, i, format(conf_mat[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_{model_name}.png'), dpi=200)\n",
    "    plt.close(fig)\n",
    "    print(f\"      ‚úÖ CM Selesai.\")\n",
    "\n",
    "# ===================================================================\n",
    "# BAGIAN 3: EKSEKUSI UTAMA\n",
    "# ===================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=== MULAI ANALISIS (INPUT MANUAL) ===\")\n",
    "    \n",
    "    for i, m in enumerate(DAFTAR_MODEL):\n",
    "        nama = m['nama']\n",
    "        log_file = m.get('log_path')\n",
    "        ckpt_file = m.get('ckpt_path')\n",
    "        folder_tujuan = m['output_folder']\n",
    "        \n",
    "        print(f\"\\n[{i+1}/{len(DAFTAR_MODEL)}] Memproses: {nama}\")\n",
    "        \n",
    "        # 1. Buat folder tujuan\n",
    "        os.makedirs(folder_tujuan, exist_ok=True)\n",
    "\n",
    "        # 2. Cek apakah file ada\n",
    "        log_ada = log_file and os.path.exists(log_file)\n",
    "        ckpt_ada = ckpt_file and os.path.exists(ckpt_file)\n",
    "\n",
    "        # 3. Copy File (Arsip)\n",
    "        if log_ada:\n",
    "            arsipkan_file(log_file, folder_tujuan)\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è Log path kosong atau tidak ditemukan: {log_file}\")\n",
    "\n",
    "        if ckpt_ada:\n",
    "            arsipkan_file(ckpt_file, folder_tujuan)\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è Ckpt path kosong atau tidak ditemukan: {ckpt_file}\")\n",
    "\n",
    "        # 4. Generate Grafik Stabilitas\n",
    "        if log_ada:\n",
    "            create_stability_curve(log_file, folder_tujuan, nama)\n",
    "        \n",
    "        # 5. Generate Confusion Matrix\n",
    "        if ckpt_ada:\n",
    "            evaluate_model(ckpt_file, folder_tujuan, nama)\n",
    "        else:\n",
    "            print(\"      ‚ö†Ô∏è SKIP Confusion Matrix karena file .pth tidak valid.\")\n",
    "            \n",
    "    print(\"\\nüéâ SEMUA PROSES SELESAI!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
