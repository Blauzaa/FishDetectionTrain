{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ee82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA tersedia? True — NVIDIA GeForce RTX 5060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmengine.config import Config\n",
    "from mmengine.runner import Runner\n",
    "from mmdet.utils import register_all_modules\n",
    "import os\n",
    "\n",
    "# ======================================================================\n",
    "#     SKRIP FINE-TUNING (TAHAP 2): VERSI EKSPERIMEN LANJUTAN\n",
    "# ======================================================================\n",
    "\n",
    "# ===== Step 1: Registrasi & Cek GPU =====\n",
    "register_all_modules()\n",
    "print(f\"CUDA tersedia? {torch.cuda.is_available()} — {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3a7a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: Menggunakan model RetinaNet untuk fine-tuning klasifikasi ---\n",
      "--- INFO: Membekukan 3 stage pertama dari backbone Swin Transformer. ---\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 2: Load Config (KONSISTEN dengan Tahap 1) =====\n",
    "print(\"--- INFO: Menggunakan model RetinaNet untuk fine-tuning klasifikasi ---\")\n",
    "#untuk retinanet\n",
    "cfg_path = 'mmdetection/configs/swin/retinanet_swin-t-p4-w7_fpn_1x_coco.py' \n",
    "#untuk faster-rcnn\n",
    "# cfg_path = 'mmdetection/configs/swin/faster-rcnn_swin-t-p4-w7_fpn_1x_coco.py'\n",
    "cfg = Config.fromfile(cfg_path)\n",
    "\n",
    "# Deteksi nama model dari path\n",
    "if \"retinanet\" in cfg_path.lower():\n",
    "    model_name = \"retinanet\"\n",
    "elif \"faster-rcnn\" in cfg_path.lower():\n",
    "    model_name = \"fasterrcnn\"\n",
    "else:\n",
    "    model_name = \"custommodel\"\n",
    "\n",
    "print(f\"--- INFO: Model yang digunakan: {model_name} ---\")\n",
    "\n",
    "# ===== [PENINGKATAN 1] Freeze Sebagian Backbone =====\n",
    "# Membekukan 3 dari 4 stage pertama dari Swin Transformer.\n",
    "# Hanya stage terakhir dan head yang akan dilatih secara intensif.\n",
    "# Ini membantu mengurangi overfitting dan menstabilkan fine-tuning.\n",
    "cfg.model.backbone.frozen_stages = 3\n",
    "print(\"--- INFO: Membekukan 3 stage pertama dari backbone Swin Transformer. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56939acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: Melakukan fine-tuning untuk 17 kelas. ---\n",
      "--- INFO: Menambahkan augmentasi PhotoMetricDistortion ke pipeline training. ---\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 3: Ganti Dataset ke Ornamental Fish & Atur Metainfo =====\n",
    "data_root = 'dataset_final/'\n",
    "\n",
    "# Definisikan nama kelas SESUAI URUTAN ID di file JSON Anda\n",
    "# CLASSES = (\n",
    "#     'Fish', 'Angel Fish', 'Betta Candy Koi Fighting Fish', 'Candy Koi Betta', \n",
    "#     'Galaxy Betta', 'Galaxy Betta Fish', 'Gemrin Koi', 'Guppy Dumbo', \n",
    "#     'Guppy Dumbo Fish', 'Juvenile Jewel', 'Juvenile Jewel Fish', 'Koi Gemrin', \n",
    "#     'Marble Molly', 'Marble Molly Fish', 'Polar Blue Parrot', 'Rancho', \n",
    "#     'Tiger Barb'\n",
    "# )\n",
    "# dua belas(12) jenis ikan\n",
    "CLASSES = (\n",
    "    'Fish','molly', 'angelfish', 'guppy', 'betta', 'cichlid',\n",
    "    'gourami', 'carp', 'goldfish', 'discus',\n",
    "    'pleco', 'barb', 'koi'\n",
    ")\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "metainfo = {'classes': CLASSES}\n",
    "print(f\"--- INFO: Melakukan fine-tuning untuk {NUM_CLASSES} kelas. ---\")\n",
    "\n",
    "# ===== [PENINGKATAN 2] Tambahkan Augmentasi Data =====\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "    dict(type='Resize', scale=(640, 640), keep_ratio=True),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='PhotoMetricDistortion'), # Menambah variasi warna & kecerahan\n",
    "    dict(type='PackDetInputs'),\n",
    "]\n",
    "val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='Resize', scale=(640, 640), keep_ratio=True),\n",
    "    dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
    "    dict(type='PackDetInputs', meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'))\n",
    "]\n",
    "print(\"--- INFO: Menambahkan augmentasi PhotoMetricDistortion ke pipeline training. ---\")\n",
    "\n",
    "# Terapkan konfigurasi ke semua dataloader\n",
    "cfg.train_dataloader.dataset.update(\n",
    "    data_root=data_root, metainfo=metainfo, pipeline=train_pipeline,\n",
    "    ann_file='train/coco.json', data_prefix=dict(img='train/'))\n",
    "cfg.val_dataloader.dataset.update(\n",
    "    data_root=data_root, metainfo=metainfo, pipeline=val_pipeline,\n",
    "    ann_file='valid/coco.json', data_prefix=dict(img='valid/'))\n",
    "cfg.test_dataloader.dataset.update(\n",
    "    data_root=data_root, metainfo=metainfo, pipeline=val_pipeline,\n",
    "    ann_file='test/coco.json', data_prefix=dict(img='test/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2539ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 4: Evaluator =====\n",
    "cfg.val_evaluator.metric = 'bbox'\n",
    "cfg.val_evaluator.ann_file = data_root + 'valid/coco.json'\n",
    "cfg.test_evaluator.metric = 'bbox'\n",
    "cfg.test_evaluator.ann_file = data_root + 'test/coco.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7f1418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: Menyetel roi_head.bbox_head.num_classes ke 17 untuk Faster R-CNN. ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ===== Step 5: Atur Kepala Model Deteksi (Model Head) =====\n",
    "# Untuk RetinaNet, head-nya berbeda dari Faster R-CNN\n",
    "# cfg.model.bbox_head.num_classes = NUM_CLASSES\n",
    "\n",
    "\n",
    "if hasattr(cfg.model, 'bbox_head') and not hasattr(cfg.model, 'roi_head'):\n",
    "    # Ini untuk arsitektur One-Stage seperti RetinaNet\n",
    "    print(f\"--- INFO: Menyetel bbox_head.num_classes ke {NUM_CLASSES} untuk RetinaNet. ---\")\n",
    "    cfg.model.bbox_head.num_classes = NUM_CLASSES\n",
    "elif hasattr(cfg.model, 'roi_head'):\n",
    "    # Ini untuk arsitektur Two-Stage seperti Faster R-CNN\n",
    "    print(f\"--- INFO: Menyetel roi_head.bbox_head.num_classes ke {NUM_CLASSES} untuk Faster R-CNN. ---\")\n",
    "    if isinstance(cfg.model.roi_head.bbox_head, list):\n",
    "        # Untuk model dengan beberapa bbox_head (seperti Cascade R-CNN)\n",
    "        for head in cfg.model.roi_head.bbox_head:\n",
    "            head.num_classes = NUM_CLASSES\n",
    "    else:\n",
    "        # Untuk Faster R-CNN standar\n",
    "        cfg.model.roi_head.bbox_head.num_classes = NUM_CLASSES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfbaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: Tidak ada checkpoint di ./outputs_swin_Faster-rcnn_ornamental_finetuned_advanced. ---\n",
      "--- INFO: Memulai training baru (fine-tuning) dari bobot pre-trained Tahap 1. ---\n",
      "PERINGATAN: Checkpoint Tahap 1 di ./outputs_swin_retinanet_deepfish/best_coco_bbox_mAP_epoch_X.pth tidak ditemukan. Memuat dari bobot Swin-COCO asli.\n"
     ]
    }
   ],
   "source": [
    "#code untuk retinanet\n",
    "\n",
    "# ===== Step 6: MELANJUTKAN Training yang Terputus =====\n",
    "\n",
    "# # [PENTING] Path ke checkpoint terakhir dari training Tahap 2 yang crash\n",
    "# checkpoint_terakhir = './outputs_swin_retinanet_ornamental_finetuned_advanced/epoch_38.pth'\n",
    "# cfg.load_from = checkpoint_terakhir\n",
    "\n",
    "# # [KUNCI UTAMA] Aktifkan mode resume\n",
    "# # Ini akan memuat state optimizer, scheduler, dan nomor epoch, lalu melanjutkan\n",
    "# cfg.resume = True\n",
    "\n",
    "# print(f\"--- INFO: Melanjutkan (RESUME) training dari: {checkpoint_terakhir} ---\")\n",
    "\n",
    "\n",
    "#code untuk faster-rcnn\n",
    "\n",
    "# ===== Step 6: Pengaturan Checkpoint & Resume yang Benar =====\n",
    "\n",
    "# Definisikan path ke folder kerja dan checkpoint yang diharapkan\n",
    "#work dir untuk retinanet swin'\n",
    "# WORK_DIR = './outputs_swin_retinanet_ornamental_finetuned_advanced'\n",
    "\n",
    "#work_dir untuk swin faster rcnn\n",
    "WORK_DIR = f'./outputs_swin_{model_name}_ornamental_finetuned_advanced'\n",
    "latest_checkpoint_path = os.path.join(WORK_DIR, 'latest.pth')\n",
    "\n",
    "# Logika utama\n",
    "if os.path.exists(latest_checkpoint_path):\n",
    "    # --- SKENARIO 1: Checkpoint DITEMUKAN, LANJUTKAN TRAINING ---\n",
    "    print(f\"--- INFO: Checkpoint terakhir ditemukan di {latest_checkpoint_path}. ---\")\n",
    "    print(\"--- INFO: Akan melanjutkan (RESUME) training. ---\")\n",
    "    cfg.load_from = latest_checkpoint_path  # Tentukan path checkpoint\n",
    "    cfg.resume = True                       # AKTIFKAN mode resume\n",
    "else:\n",
    "    # --- SKENARIO 2: TIDAK ADA CHECKPOINT, MULAI DARI AWAL (PRE-TRAINED) ---\n",
    "    print(f\"--- INFO: Tidak ada checkpoint di {WORK_DIR}. ---\")\n",
    "    print(\"--- INFO: Memulai training baru (fine-tuning) dari bobot pre-trained Tahap 1. ---\")\n",
    "    \n",
    "    # Ganti dengan path checkpoint dari TAHAP 1 (pre-training di DeepFish)\n",
    "    tahap_1_checkpoint = './outputs_swin_retinanet_deepfish/best_coco_bbox_mAP_epoch_X.pth' # <-- GANTI INI\n",
    "    \n",
    "    if os.path.exists(tahap_1_checkpoint):\n",
    "        cfg.load_from = tahap_1_checkpoint\n",
    "    else:\n",
    "        # Jika checkpoint tahap 1 juga tidak ada, bisa set ke None atau URL Swin asli\n",
    "        print(f\"PERINGATAN: Checkpoint Tahap 1 di {tahap_1_checkpoint} tidak ditemukan. Memuat dari bobot Swin-COCO asli.\")\n",
    "        # Biarkan cfg.load_from default dari file config (bobot pre-trained di COCO)\n",
    "    \n",
    "    cfg.resume = False # PASTIKAN resume false untuk training baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d205e102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFO: Training diatur untuk 50 epoch. ---\n",
      "--- INFO: Menggunakan MultiStepLR scheduler, LR akan turun di epoch 35 & 45. ---\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 7: Konfigurasi Training (Fine-tuning Lanjutan) =====\n",
    "cfg.optim_wrapper.optimizer.lr = 0.00001  # Mulai dengan LR yang terbukti baik\n",
    "cfg.train_cfg.max_epochs = 50  # Tingkatkan total epoch\n",
    "print(f\"--- INFO: Training diatur untuk {cfg.train_cfg.max_epochs} epoch. ---\")\n",
    "\n",
    "# ===== [PENINGKATAN 3] Jadwal Training yang Lebih Panjang =====\n",
    "# Gunakan scheduler yang menurunkan LR di tahap akhir untuk penyempurnaan\n",
    "cfg.param_scheduler = [\n",
    "    dict(type='LinearLR', start_factor=0.001, by_epoch=False, begin=0, end=500), # Warmup\n",
    "    dict(\n",
    "        type='MultiStepLR',\n",
    "        by_epoch=True,\n",
    "        begin=0,\n",
    "        end=50,\n",
    "        milestones=[35, 45], # Turunkan LR di epoch 35 dan 45\n",
    "        gamma=0.1)\n",
    "]\n",
    "print(\"--- INFO: Menggunakan MultiStepLR scheduler, LR akan turun di epoch 35 & 45. ---\")\n",
    "\n",
    "\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "cfg.default_hooks.checkpoint.interval = 2 # Simpan setiap 2 epoch\n",
    "cfg.default_hooks.checkpoint.max_keep_ckpts = 3\n",
    "cfg.default_hooks.checkpoint.save_best = 'coco/bbox_mAP'\n",
    "cfg.default_hooks.checkpoint.rule = 'greater'\n",
    "cfg.visualizer.vis_backends = [dict(type='LocalVisBackend'), dict(type='TensorboardVisBackend')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271e399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 8: Output Folder Baru =====\n",
    "# Gunakan nama folder baru untuk eksperimen ini\n",
    "cfg.work_dir = WORK_DIR\n",
    "os.makedirs(cfg.work_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da08a46f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\n",
      "Jumlah data fine-tuning: 2440\n",
      "Kelas yang akan dilatih: ('Fish', 'Angel Fish', 'Betta Candy Koi Fighting Fish', 'Candy Koi Betta', 'Galaxy Betta', 'Galaxy Betta Fish', 'Gemrin Koi', 'Guppy Dumbo', 'Guppy Dumbo Fish', 'Juvenile Jewel', 'Juvenile Jewel Fish', 'Koi Gemrin', 'Marble Molly', 'Marble Molly Fish', 'Polar Blue Parrot', 'Rancho', 'Tiger Barb')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Step 9: Verifikasi Dataset =====\n",
    "from mmengine.registry import DATASETS\n",
    "train_dataset = DATASETS.build(cfg.train_dataloader.dataset)\n",
    "print(f\"\\nJumlah data fine-tuning: {len(train_dataset)}\")\n",
    "print(f\"Kelas yang akan dilatih: {train_dataset.metainfo['classes']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d327a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08/13 01:24:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: win32\n",
      "    Python: 3.10.18 | packaged by Anaconda, Inc. | (main, Jun  5 2025, 13:08:55) [MSC v.1929 64 bit (AMD64)]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1393167610\n",
      "    GPU 0: NVIDIA GeForce RTX 5060\n",
      "    CUDA_HOME: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.8\n",
      "    NVCC: Cuda compilation tools, release 12.8, V12.8.61\n",
      "    MSVC: n/a, reason: fileno\n",
      "    PyTorch: 2.8.0+cu128\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - C++ Version: 201703\n",
      "  - MSVC 193833145\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2025.2-Product Build 20250620 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.7.1 (Git Hash 8d263e693366ef8db40acc569cc7d8edf644556d)\n",
      "  - OpenMP 2019\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 12.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90;-gencode;arch=compute_100,code=sm_100;-gencode;arch=compute_120,code=sm_120\n",
      "  - CuDNN 91.0.2  (built against CUDA 12.9)\n",
      "  - Magma 2.5.4\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, COMMIT_SHA=a1cb3cc05d46d198467bebbb6e8fba50a325d4e7, CUDA_VERSION=12.8, CUDNN_VERSION=9.10.2, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/pytorch/.ci/pytorch/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /EHsc /Zc:__cplusplus /bigobj /FS /utf-8 -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DLIBKINETO_NOXPUPTI=ON -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE /wd4624 /wd4068 /wd4067 /wd4267 /wd4661 /wd4717 /wd4244 /wd4804 /wd4273, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, TORCH_VERSION=2.8.0, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=OFF, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, USE_XCCL=OFF, USE_XPU=OFF, \n",
      "\n",
      "    TorchVision: 0.23.0+cu128\n",
      "    OpenCV: 4.12.0\n",
      "    MMEngine: 0.10.7\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: False\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1393167610\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "08/13 01:24:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Config:\n",
      "auto_scale_lr = dict(base_batch_size=16, enable=False)\n",
      "backend_args = None\n",
      "data_root = 'path/to/your/fish_dataset/'\n",
      "dataset_type = 'CocoDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        interval=2,\n",
      "        max_keep_ckpts=3,\n",
      "        rule='greater',\n",
      "        save_best='coco/bbox_mAP',\n",
      "        type='CheckpointHook'),\n",
      "    logger=dict(interval=10, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='DetVisualizationHook'))\n",
      "default_scope = 'mmdet'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=False,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=True, type='LogProcessor', window_size=50)\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'Cupang',\n",
      "        'Guppy',\n",
      "        'Neon Tetra',\n",
      "        'Mas Koki',\n",
      "        'Manfish',\n",
      "    ))\n",
      "model = dict(\n",
      "    backbone=dict(\n",
      "        attn_drop_rate=0.0,\n",
      "        convert_weights=True,\n",
      "        depths=[\n",
      "            2,\n",
      "            2,\n",
      "            6,\n",
      "            2,\n",
      "        ],\n",
      "        drop_path_rate=0.2,\n",
      "        drop_rate=0.0,\n",
      "        embed_dims=96,\n",
      "        frozen_stages=3,\n",
      "        init_cfg=dict(\n",
      "            checkpoint=\n",
      "            'https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth',\n",
      "            type='Pretrained'),\n",
      "        mlp_ratio=4,\n",
      "        num_heads=[\n",
      "            3,\n",
      "            6,\n",
      "            12,\n",
      "            24,\n",
      "        ],\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        patch_norm=True,\n",
      "        qk_scale=None,\n",
      "        qkv_bias=True,\n",
      "        type='SwinTransformer',\n",
      "        window_size=7,\n",
      "        with_cp=False),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_size_divisor=32,\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='DetDataPreprocessor'),\n",
      "    neck=dict(\n",
      "        in_channels=[\n",
      "            96,\n",
      "            192,\n",
      "            384,\n",
      "            768,\n",
      "        ],\n",
      "        num_outs=5,\n",
      "        out_channels=256,\n",
      "        type='FPN'),\n",
      "    roi_head=dict(\n",
      "        bbox_head=dict(\n",
      "            bbox_coder=dict(\n",
      "                target_means=[\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                    0.0,\n",
      "                ],\n",
      "                target_stds=[\n",
      "                    0.1,\n",
      "                    0.1,\n",
      "                    0.2,\n",
      "                    0.2,\n",
      "                ],\n",
      "                type='DeltaXYWHBBoxCoder'),\n",
      "            fc_out_channels=1024,\n",
      "            in_channels=256,\n",
      "            loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "            loss_cls=dict(\n",
      "                loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "            num_classes=17,\n",
      "            reg_class_agnostic=False,\n",
      "            roi_feat_size=7,\n",
      "            type='Shared2FCBBoxHead'),\n",
      "        bbox_roi_extractor=dict(\n",
      "            featmap_strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "            ],\n",
      "            out_channels=256,\n",
      "            roi_layer=dict(output_size=7, sampling_ratio=0, type='RoIAlign'),\n",
      "            type='SingleRoIExtractor'),\n",
      "        type='StandardRoIHead'),\n",
      "    rpn_head=dict(\n",
      "        anchor_generator=dict(\n",
      "            ratios=[\n",
      "                0.5,\n",
      "                1.0,\n",
      "                2.0,\n",
      "            ],\n",
      "            scales=[\n",
      "                8,\n",
      "            ],\n",
      "            strides=[\n",
      "                4,\n",
      "                8,\n",
      "                16,\n",
      "                32,\n",
      "                64,\n",
      "            ],\n",
      "            type='AnchorGenerator'),\n",
      "        bbox_coder=dict(\n",
      "            target_means=[\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "                0.0,\n",
      "            ],\n",
      "            target_stds=[\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "                1.0,\n",
      "            ],\n",
      "            type='DeltaXYWHBBoxCoder'),\n",
      "        feat_channels=256,\n",
      "        in_channels=256,\n",
      "        loss_bbox=dict(loss_weight=1.0, type='L1Loss'),\n",
      "        loss_cls=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=True),\n",
      "        type='RPNHead'),\n",
      "    test_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            max_per_img=100,\n",
      "            nms=dict(iou_threshold=0.5, type='nms'),\n",
      "            score_thr=0.05),\n",
      "        rpn=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=1000)),\n",
      "    train_cfg=dict(\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=False,\n",
      "                min_pos_iou=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                pos_iou_thr=0.5,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=True,\n",
      "                neg_pos_ub=-1,\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                type='RandomSampler')),\n",
      "        rpn=dict(\n",
      "            allowed_border=-1,\n",
      "            assigner=dict(\n",
      "                ignore_iof_thr=-1,\n",
      "                match_low_quality=True,\n",
      "                min_pos_iou=0.3,\n",
      "                neg_iou_thr=0.3,\n",
      "                pos_iou_thr=0.7,\n",
      "                type='MaxIoUAssigner'),\n",
      "            debug=False,\n",
      "            pos_weight=-1,\n",
      "            sampler=dict(\n",
      "                add_gt_as_proposals=False,\n",
      "                neg_pos_ub=-1,\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                type='RandomSampler')),\n",
      "        rpn_proposal=dict(\n",
      "            max_per_img=1000,\n",
      "            min_bbox_size=0,\n",
      "            nms=dict(iou_threshold=0.7, type='nms'),\n",
      "            nms_pre=2000)),\n",
      "    type='FasterRCNN')\n",
      "optim_wrapper = dict(\n",
      "    optimizer=dict(lr=1e-05, momentum=0.9, type='SGD', weight_decay=0.0001),\n",
      "    type='OptimWrapper')\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0, by_epoch=False, end=500, start_factor=0.001, type='LinearLR'),\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=True,\n",
      "        end=50,\n",
      "        gamma=0.1,\n",
      "        milestones=[\n",
      "            35,\n",
      "            45,\n",
      "        ],\n",
      "        type='MultiStepLR'),\n",
      "]\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='test/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='test/'),\n",
      "        data_root='dataset/Ornamental Freshwater Fish/',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Fish',\n",
      "                'Angel Fish',\n",
      "                'Betta Candy Koi Fighting Fish',\n",
      "                'Candy Koi Betta',\n",
      "                'Galaxy Betta',\n",
      "                'Galaxy Betta Fish',\n",
      "                'Gemrin Koi',\n",
      "                'Guppy Dumbo',\n",
      "                'Guppy Dumbo Fish',\n",
      "                'Juvenile Jewel',\n",
      "                'Juvenile Jewel Fish',\n",
      "                'Koi Gemrin',\n",
      "                'Marble Molly',\n",
      "                'Marble Molly Fish',\n",
      "                'Polar Blue Parrot',\n",
      "                'Rancho',\n",
      "                'Tiger Barb',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                640,\n",
      "                640,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    ann_file='dataset/Ornamental Freshwater Fish/test/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "test_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(\n",
      "        meta_keys=(\n",
      "            'img_id',\n",
      "            'img_path',\n",
      "            'ori_shape',\n",
      "            'img_shape',\n",
      "            'scale_factor',\n",
      "        ),\n",
      "        type='PackDetInputs'),\n",
      "]\n",
      "train_cfg = dict(max_epochs=50, type='EpochBasedTrainLoop', val_interval=1)\n",
      "train_dataloader = dict(\n",
      "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='train/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='train/'),\n",
      "        data_root='dataset/Ornamental Freshwater Fish/',\n",
      "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Fish',\n",
      "                'Angel Fish',\n",
      "                'Betta Candy Koi Fighting Fish',\n",
      "                'Candy Koi Betta',\n",
      "                'Galaxy Betta',\n",
      "                'Galaxy Betta Fish',\n",
      "                'Gemrin Koi',\n",
      "                'Guppy Dumbo',\n",
      "                'Guppy Dumbo Fish',\n",
      "                'Juvenile Jewel',\n",
      "                'Juvenile Jewel Fish',\n",
      "                'Koi Gemrin',\n",
      "                'Marble Molly',\n",
      "                'Marble Molly Fish',\n",
      "                'Polar Blue Parrot',\n",
      "                'Rancho',\n",
      "                'Tiger Barb',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                640,\n",
      "                640,\n",
      "            ), type='Resize'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PhotoMetricDistortion'),\n",
      "            dict(type='PackDetInputs'),\n",
      "        ],\n",
      "        type='CocoDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='DefaultSampler'))\n",
      "train_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        1333,\n",
      "        800,\n",
      "    ), type='Resize'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackDetInputs'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='valid/_annotations.coco.json',\n",
      "        backend_args=None,\n",
      "        data_prefix=dict(img='valid/'),\n",
      "        data_root='dataset/Ornamental Freshwater Fish/',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'Fish',\n",
      "                'Angel Fish',\n",
      "                'Betta Candy Koi Fighting Fish',\n",
      "                'Candy Koi Betta',\n",
      "                'Galaxy Betta',\n",
      "                'Galaxy Betta Fish',\n",
      "                'Gemrin Koi',\n",
      "                'Guppy Dumbo',\n",
      "                'Guppy Dumbo Fish',\n",
      "                'Juvenile Jewel',\n",
      "                'Juvenile Jewel Fish',\n",
      "                'Koi Gemrin',\n",
      "                'Marble Molly',\n",
      "                'Marble Molly Fish',\n",
      "                'Polar Blue Parrot',\n",
      "                'Rancho',\n",
      "                'Tiger Barb',\n",
      "            )),\n",
      "        pipeline=[\n",
      "            dict(backend_args=None, type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                640,\n",
      "                640,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=False),\n",
      "            dict(\n",
      "                meta_keys=(\n",
      "                    'img_id',\n",
      "                    'img_path',\n",
      "                    'ori_shape',\n",
      "                    'img_shape',\n",
      "                    'scale_factor',\n",
      "                ),\n",
      "                type='PackDetInputs'),\n",
      "        ],\n",
      "        test_mode=True,\n",
      "        type='CocoDataset'),\n",
      "    drop_last=False,\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    ann_file='dataset/Ornamental Freshwater Fish/valid/_annotations.coco.json',\n",
      "    backend_args=None,\n",
      "    format_only=False,\n",
      "    metric='bbox',\n",
      "    type='CocoMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='DetLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "        dict(type='TensorboardVisBackend'),\n",
      "    ])\n",
      "work_dir = './outputs_swin_Faster-rcnn_ornamental_finetuned_advanced'\n",
      "\n",
      "08/13 01:24:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "08/13 01:24:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DetVisualizationHook               \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "08/13 01:24:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Loads checkpoint by http backend from path: https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth\n",
      "08/13 01:24:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "08/13 01:24:44 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"HardDiskBackend\" is the alias of \"LocalBackend\" and the former will be deprecated in future.\n",
      "08/13 01:24:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to c:\\Users\\Stevenstven\\Documents\\VSCode\\Skripsi\\Deteksi_Jenis_Ikan\\outputs_swin_Faster-rcnn_ornamental_finetuned_advanced.\n",
      "08/13 01:24:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][  10/1220]  lr: 1.9018e-07  eta: 18:09:29  time: 1.0718  data_time: 0.8368  memory: 1022  loss: 3.8456  loss_rpn_cls: 0.6801  loss_rpn_bbox: 0.0123  loss_cls: 3.1510  acc: 0.0000  loss_bbox: 0.0022\n",
      "08/13 01:24:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][  20/1220]  lr: 3.9038e-07  eta: 10:25:07  time: 0.6151  data_time: 0.4197  memory: 1022  loss: 3.8558  loss_rpn_cls: 0.6802  loss_rpn_bbox: 0.0131  loss_cls: 3.1581  acc: 0.4883  loss_bbox: 0.0044\n",
      "08/13 01:24:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train)  [1][  30/1220]  lr: 5.9058e-07  eta: 7:50:41  time: 0.4632  data_time: 0.2808  memory: 1022  loss: 3.8419  loss_rpn_cls: 0.6806  loss_rpn_bbox: 0.0124  loss_cls: 3.1445  acc: 0.0000  loss_bbox: 0.0045\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ===== Step 10: Mulai Fine-tuning =====\u001b[39;00m\n\u001b[0;32m      2\u001b[0m runner \u001b[38;5;241m=\u001b[39m Runner\u001b[38;5;241m.\u001b[39mfrom_cfg(cfg)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>> Proses fine-tuning selesai.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\runner\\runner.py:1777\u001b[0m, in \u001b[0;36mRunner.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[38;5;66;03m# Maybe compile the model according to options in self.cfg.compile\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;66;03m# This must be called **AFTER** model has been wrapped.\u001b[39;00m\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_compile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_step\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1777\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\runner\\loops.py:98\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decide_current_val_interval()\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mval_loop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    102\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_begin\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    104\u001b[0m                  \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs)):\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\runner\\loops.py:115\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, data_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataloader):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\runner\\loops.py:131\u001b[0m, in \u001b[0;36mEpochBasedTrainLoop.run_iter\u001b[1;34m(self, idx, data_batch)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_idx\u001b[38;5;241m=\u001b[39midx, data_batch\u001b[38;5;241m=\u001b[39mdata_batch)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Enable gradient accumulation mode and avoid unnecessary gradient\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# synchronization during gradient accumulation process.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# outputs should be a dict of loss.\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim_wrapper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunner\u001b[38;5;241m.\u001b[39mcall_hook(\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    136\u001b[0m     batch_idx\u001b[38;5;241m=\u001b[39midx,\n\u001b[0;32m    137\u001b[0m     data_batch\u001b[38;5;241m=\u001b[39mdata_batch,\n\u001b[0;32m    138\u001b[0m     outputs\u001b[38;5;241m=\u001b[39moutputs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\model\\base_model\\base_model.py:114\u001b[0m, in \u001b[0;36mBaseModel.train_step\u001b[1;34m(self, data, optim_wrapper)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m optim_wrapper\u001b[38;5;241m.\u001b[39moptim_context(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    113\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_preprocessor(data, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 114\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    115\u001b[0m parsed_losses, log_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_losses(losses)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    116\u001b[0m optim_wrapper\u001b[38;5;241m.\u001b[39mupdate_params(parsed_losses)\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmengine\\model\\base_model\\base_model.py:361\u001b[0m, in \u001b[0;36mBaseModel._run_forward\u001b[1;34m(self, data, mode)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Unpacks data for :meth:`forward`\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;124;03m    dict or list: Results of training or testing mode.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 361\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    363\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39mdata, mode\u001b[38;5;241m=\u001b[39mmode)\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\detectors\\base.py:92\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[1;34m(self, inputs, data_samples, mode)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The unified entry for a forward process in both training and test.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03mThe method should accept three modes: \"tensor\", \"predict\" and \"loss\":\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;124;03m    - If ``mode=\"loss\"``, return a dict of tensor.\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredict\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(inputs, data_samples)\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\detectors\\two_stage.py:174\u001b[0m, in \u001b[0;36mTwoStageDetector.loss\u001b[1;34m(self, batch_inputs, batch_data_samples)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_sample \u001b[38;5;129;01min\u001b[39;00m rpn_data_samples:\n\u001b[0;32m    171\u001b[0m     data_sample\u001b[38;5;241m.\u001b[39mgt_instances\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    172\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros_like(data_sample\u001b[38;5;241m.\u001b[39mgt_instances\u001b[38;5;241m.\u001b[39mlabels)\n\u001b[1;32m--> 174\u001b[0m rpn_losses, rpn_results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn_head\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_and_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrpn_data_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposal_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproposal_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# avoid get same name with roi_head loss\u001b[39;00m\n\u001b[0;32m    177\u001b[0m keys \u001b[38;5;241m=\u001b[39m rpn_losses\u001b[38;5;241m.\u001b[39mkeys()\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\dense_heads\\base_dense_head.py:165\u001b[0m, in \u001b[0;36mBaseDenseHead.loss_and_predict\u001b[1;34m(self, x, batch_data_samples, proposal_cfg)\u001b[0m\n\u001b[0;32m    161\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[0;32m    163\u001b[0m loss_inputs \u001b[38;5;241m=\u001b[39m outs \u001b[38;5;241m+\u001b[39m (batch_gt_instances, batch_img_metas,\n\u001b[0;32m    164\u001b[0m                       batch_gt_instances_ignore)\n\u001b[1;32m--> 165\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_by_feat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloss_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_by_feat(\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;241m*\u001b[39mouts, batch_img_metas\u001b[38;5;241m=\u001b[39mbatch_img_metas, cfg\u001b[38;5;241m=\u001b[39mproposal_cfg)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m losses, predictions\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\dense_heads\\rpn_head.py:125\u001b[0m, in \u001b[0;36mRPNHead.loss_by_feat\u001b[1;34m(self, cls_scores, bbox_preds, batch_gt_instances, batch_img_metas, batch_gt_instances_ignore)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_by_feat\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    100\u001b[0m                  cls_scores: List[Tensor],\n\u001b[0;32m    101\u001b[0m                  bbox_preds: List[Tensor],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m                  batch_gt_instances_ignore: OptInstanceList \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \\\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculate the loss based on the features extracted by the detection\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03m    head.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;124;03m        dict[str, Tensor]: A dictionary of loss components.\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_by_feat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcls_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbbox_preds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_gt_instances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_gt_instances_ignore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_gt_instances_ignore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    132\u001b[0m         loss_rpn_cls\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_cls\u001b[39m\u001b[38;5;124m'\u001b[39m], loss_rpn_bbox\u001b[38;5;241m=\u001b[39mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\dense_heads\\anchor_head.py:500\u001b[0m, in \u001b[0;36mAnchorHead.loss_by_feat\u001b[1;34m(self, cls_scores, bbox_preds, batch_gt_instances, batch_img_metas, batch_gt_instances_ignore)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(featmap_sizes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprior_generator\u001b[38;5;241m.\u001b[39mnum_levels\n\u001b[0;32m    498\u001b[0m device \u001b[38;5;241m=\u001b[39m cls_scores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 500\u001b[0m anchor_list, valid_flag_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_anchors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatmap_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_img_metas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m cls_reg_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_targets(\n\u001b[0;32m    503\u001b[0m     anchor_list,\n\u001b[0;32m    504\u001b[0m     valid_flag_list,\n\u001b[0;32m    505\u001b[0m     batch_gt_instances,\n\u001b[0;32m    506\u001b[0m     batch_img_metas,\n\u001b[0;32m    507\u001b[0m     batch_gt_instances_ignore\u001b[38;5;241m=\u001b[39mbatch_gt_instances_ignore)\n\u001b[0;32m    508\u001b[0m (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n\u001b[0;32m    509\u001b[0m  avg_factor) \u001b[38;5;241m=\u001b[39m cls_reg_targets\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\dense_heads\\anchor_head.py:188\u001b[0m, in \u001b[0;36mAnchorHead.get_anchors\u001b[1;34m(self, featmap_sizes, batch_img_metas, device)\u001b[0m\n\u001b[0;32m    184\u001b[0m num_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_img_metas)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# since feature map sizes of all images are the same, we only compute\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# anchors for one time\u001b[39;00m\n\u001b[1;32m--> 188\u001b[0m multi_level_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprior_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_priors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatmap_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m anchor_list \u001b[38;5;241m=\u001b[39m [multi_level_anchors \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_imgs)]\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# for each image, we compute valid flags of multi level anchors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\task_modules\\prior_generators\\anchor_generator.py:254\u001b[0m, in \u001b[0;36mAnchorGenerator.grid_priors\u001b[1;34m(self, featmap_sizes, dtype, device)\u001b[0m\n\u001b[0;32m    252\u001b[0m multi_level_anchors \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_levels):\n\u001b[1;32m--> 254\u001b[0m     anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_level_grid_priors\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatmap_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     multi_level_anchors\u001b[38;5;241m.\u001b[39mappend(anchors)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multi_level_anchors\n",
      "File \u001b[1;32mc:\\Users\\Stevenstven\\anaconda3\\envs\\ikan_env\\lib\\site-packages\\mmdet\\models\\task_modules\\prior_generators\\anchor_generator.py:281\u001b[0m, in \u001b[0;36mAnchorGenerator.single_level_grid_priors\u001b[1;34m(self, featmap_size, level_idx, dtype, device)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msingle_level_grid_priors\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    260\u001b[0m                              featmap_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    261\u001b[0m                              level_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    262\u001b[0m                              dtype: torch\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m    263\u001b[0m                              device: DeviceType \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    264\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate grid anchors of a single level.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;124;03m        torch.Tensor: Anchors in the overall feature maps.\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 281\u001b[0m     base_anchors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_anchors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(dtype)\n\u001b[0;32m    282\u001b[0m     feat_h, feat_w \u001b[38;5;241m=\u001b[39m featmap_size\n\u001b[0;32m    283\u001b[0m     stride_w, stride_h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrides[level_idx]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===== Step 10: Mulai Fine-tuning =====\n",
    "runner = Runner.from_cfg(cfg)\n",
    "runner.train()\n",
    "print(\">>> Proses fine-tuning selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a9613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Memulai inference pada gambar tes...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Cari checkpoint terbaik\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m best_ckpts \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(cfg\u001b[38;5;241m.\u001b[39mwork_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_ckpts:\n\u001b[0;32m     10\u001b[0m     checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39mwork_dir, \u001b[38;5;28msorted\u001b[39m(best_ckpts)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# ===== Step 11: Inference Otomatis + Rekap Deteksi =====\n",
    "print(\"\\n>>> Memulai inference pada gambar tes...\")\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmdet.visualization import DetLocalVisualizer\n",
    "import mmcv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Cari checkpoint terbaik\n",
    "best_ckpts = [f for f in os.listdir(cfg.work_dir) if f.startswith('best_') and f.endswith('.pth')]\n",
    "if best_ckpts:\n",
    "    checkpoint_path = os.path.join(cfg.work_dir, sorted(best_ckpts)[-1])\n",
    "else:\n",
    "    checkpoint_path = os.path.join(cfg.work_dir, \"latest.pth\")\n",
    "\n",
    "print(f\"--- INFO: Menggunakan checkpoint: {checkpoint_path} ---\")\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    model = init_detector(cfg, checkpoint_path, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    visualizer = DetLocalVisualizer(vis_backends=[dict(type='LocalVisBackend')])\n",
    "    visualizer.dataset_meta = cfg.train_dataloader.dataset.metainfo\n",
    "\n",
    "    img_path = 'dataset/Ornamental Freshwater Fish/test/IMG_0194_jpg.rf.2da382821c1042b160f37a96548228de.jpg'\n",
    "    output_folder = os.path.join(cfg.work_dir, 'test_outputs')\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, 'result_image.jpg')\n",
    "\n",
    "    if os.path.exists(img_path):\n",
    "        # Jalankan inference\n",
    "        result = inference_detector(model, img_path)\n",
    "\n",
    "        # Ambil nama kelas\n",
    "        class_names = cfg.train_dataloader.dataset.metainfo['classes']\n",
    "\n",
    "        # Ambil prediksi bbox dari result\n",
    "        pred_instances = result.pred_instances\n",
    "        labels = pred_instances.labels.cpu().numpy()\n",
    "        scores = pred_instances.scores.cpu().numpy()\n",
    "\n",
    "        # Terapkan threshold skor\n",
    "        score_thr = 0.4\n",
    "        valid_idx = scores >= score_thr\n",
    "        labels = labels[valid_idx]\n",
    "\n",
    "        # Hitung jumlah per kelas\n",
    "        counts = Counter(labels)\n",
    "\n",
    "        # Simpan gambar hasil deteksi\n",
    "        img = mmcv.imread(img_path)\n",
    "        img = mmcv.imconvert(img, 'bgr', 'rgb')\n",
    "        visualizer.add_datasample(\n",
    "            name='Hasil Deteksi',\n",
    "            image=img,\n",
    "            data_sample=result,\n",
    "            draw_gt=False,\n",
    "            show=False,\n",
    "            pred_score_thr=score_thr,\n",
    "            out_file=output_path\n",
    "        )\n",
    "        print(f\">>> Inference selesai. Hasil tersimpan di {output_path}\")\n",
    "\n",
    "        # Cetak rekap\n",
    "        print(\"\\n==== Summary Deteksi ====\")\n",
    "        if counts:\n",
    "            for cls_id, count in counts.items():\n",
    "                print(f\"{class_names[cls_id]} = {count}\")\n",
    "        else:\n",
    "            print(\"Tidak ada ikan terdeteksi.\")\n",
    "    else:\n",
    "        print(f\"ERROR: Gambar uji tidak ditemukan di {img_path}\")\n",
    "else:\n",
    "    print(\"ERROR: Tidak ada checkpoint untuk inference.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
