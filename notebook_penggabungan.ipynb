{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cdaf3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tes Ketersediaan GPU PyTorch ---\n",
      "Apakah CUDA tersedia? -> True\n",
      "Jumlah GPU yang terdeteksi: 1\n",
      "Nama GPU: NVIDIA GeForce RTX 5060\n",
      "\n",
      "--- Tes Penggunaan Memori GPU ---\n",
      "Tensor dibuat di device: cpu\n",
      "Tensor berhasil dipindahkan ke device: cuda:0\n",
      "Isi tensor di GPU:\n",
      "tensor([[ 0.7970, -0.0774,  1.1627],\n",
      "        [-0.1303, -1.3215,  0.0492],\n",
      "        [ 0.6236, -0.2506, -0.4992],\n",
      "        [-0.5724,  0.1147, -1.4978],\n",
      "        [-0.2497, -0.7462, -0.7004]], device='cuda:0')\n",
      "\n",
      "‚úÖ Tes GPU berhasil! PyTorch bisa menggunakan GPU Anda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"--- Tes Ketersediaan GPU PyTorch ---\")\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"Apakah CUDA tersedia? -> {is_available}\")\n",
    "\n",
    "if not is_available:\n",
    "    print(\"!! Peringatan: PyTorch tidak dapat mendeteksi GPU Anda. Pastikan driver NVIDIA dan CUDA Toolkit sudah benar.\")\n",
    "else:\n",
    "            # 2. Dapatkan jumlah GPU yang terdeteksi\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Jumlah GPU yang terdeteksi: {gpu_count}\")\n",
    "\n",
    "            # 3. Dapatkan nama GPU yang sedang aktif (GPU #0)\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"Nama GPU: {gpu_name}\")\n",
    "\n",
    "            # 4. Tes membuat tensor dan memindahkannya ke GPU\n",
    "    print(\"\\n--- Tes Penggunaan Memori GPU ---\")\n",
    "    try:\n",
    "                # Buat sebuah tensor sederhana di CPU\n",
    "        cpu_tensor = torch.randn(5, 3)\n",
    "        print(f\"Tensor dibuat di device: {cpu_tensor.device}\")\n",
    "\n",
    "                # Pindahkan tensor ke GPU\n",
    "        gpu_tensor = cpu_tensor.to('cuda')\n",
    "        print(f\"Tensor berhasil dipindahkan ke device: {gpu_tensor.device}\")\n",
    "        print(\"Isi tensor di GPU:\")\n",
    "        print(gpu_tensor)\n",
    "\n",
    "        print(\"\\n‚úÖ Tes GPU berhasil! PyTorch bisa menggunakan GPU Anda.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi error saat mencoba menggunakan GPU: {e}\")\n",
    "        print(\"\\n‚ùå Tes GPU gagal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9de4c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 5060\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd55716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FASE 1: DEFINISIKAN RENCANA HARMONISASI ANDA\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Daftar kelas master final yang Anda inginkan. ID harus dimulai dari 0.\n",
    "MASTER_CATEGORIES = [\n",
    "    {\"id\": 0, \"name\": \"Angel/Manfish\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 1, \"name\": \"Betta\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 2, \"name\": \"Goldfish\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 3, \"name\": \"Koi\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 4, \"name\": \"Tetra\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 5, \"name\": \"Botia\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 6, \"name\": \"Molly\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 7, \"name\": \"Guppy\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 8, \"name\": \"Rainbow\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 9, \"name\": \"Komet\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 10, \"name\": \"Lemon Cichlid\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 11, \"name\": \"Jewel Cichlid\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 12, \"name\": \"Polar Blue Parrot\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 13, \"name\": \"Tiger Barb\", \"supercategory\": \"fish\"}\n",
    "]\n",
    "\n",
    "# Peta konversi dari ID dataset lama ke ID master BARU.\n",
    "map_d1 = {\n",
    "    1: 4, 2: 5, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3,\n",
    "    11: 3, 12: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 2, 18: 2, 19: 2,\n",
    "    20: 2, 21: 2, 22: 9, 23: 10, 24: 0, 25: 8\n",
    "}\n",
    "map_d2 = {\n",
    "    1: 0, 2: 1, 3: 1, 4: 3, 5: 7, 6: 11, 7: 6, 8: 12, 9: 2, 10: 13\n",
    "}\n",
    "\n",
    "# Path dasar ke setiap dataset\n",
    "datasets_info = [\n",
    "    {\n",
    "        \"base_path\": 'Dataset/Ornamental Fish',\n",
    "        \"id_map\": map_d1\n",
    "    },\n",
    "    {\n",
    "        \"base_path\": 'Dataset/Ornamental Freshwater Fish',\n",
    "        \"id_map\": map_d2\n",
    "    }\n",
    "]\n",
    "\n",
    "# Path output untuk dataset gabungan\n",
    "output_path = './final_combined_dataset/'\n",
    "\n",
    "print(\"Konfigurasi selesai. Siap untuk Fase 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FASE 2: FUNGSI DAN EKSEKUSI PENGGABUNGAN\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def merge_coco_datasets_all_splits(datasets, master_categories, output_base_path):\n",
    "    merged_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": master_categories,\n",
    "        \"info\": {\"description\": \"Merged Dataset from multiple sources.\"},\n",
    "        \"licenses\": []\n",
    "    }\n",
    "\n",
    "    image_id_offset = 0\n",
    "    annotation_id_offset = 0\n",
    "    \n",
    "    # Buat folder sementara untuk semua gambar\n",
    "    temp_images_all_path = os.path.join(output_base_path, 'images_all')\n",
    "    os.makedirs(temp_images_all_path, exist_ok=True)\n",
    "    print(f\"Created temporary directory: {temp_images_all_path}\")\n",
    "\n",
    "    for dataset_info in datasets:\n",
    "        base_path = dataset_info[\"base_path\"]\n",
    "        id_map = dataset_info[\"id_map\"]\n",
    "        print(f\"\\nProcessing dataset at base path: {base_path}\")\n",
    "        \n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            json_path = os.path.join(base_path, split, '_annotations.coco.json')\n",
    "            \n",
    "            if not os.path.exists(json_path):\n",
    "                print(f\"  - Split '{split}' not found in '{base_path}', skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Path gambar bisa berada langsung di dalam folder split atau di subfolder 'images'\n",
    "            image_dir = os.path.join(base_path, split)\n",
    "            \n",
    "            print(f\"  + Merging split: '{split}'\")\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            image_filename_map = {img['id']: img['file_name'] for img in data['images']}\n",
    "            \n",
    "            current_image_id_map = {}\n",
    "            for old_img_id, filename in image_filename_map.items():\n",
    "                new_img_id = len(merged_data['images'])\n",
    "                current_image_id_map[old_img_id] = new_img_id\n",
    "                \n",
    "                # Cari detail gambar\n",
    "                img_details = next((item for item in data['images'] if item[\"id\"] == old_img_id), None)\n",
    "                if img_details:\n",
    "                    img_details['id'] = new_img_id\n",
    "                    merged_data['images'].append(img_details)\n",
    "                \n",
    "                # Salin file gambar\n",
    "                src_img_path = os.path.join(image_dir, filename) # Roboflow V2 format\n",
    "                if not os.path.exists(src_img_path): # Coba format lama jika tidak ditemukan\n",
    "                    src_img_path = os.path.join(image_dir, 'images', filename)\n",
    "\n",
    "                if os.path.exists(src_img_path):\n",
    "                    shutil.copy2(src_img_path, os.path.join(temp_images_all_path, filename))\n",
    "                else:\n",
    "                    print(f\"    [WARNING] Image not found, skipping copy: {src_img_path}\")\n",
    "\n",
    "            # Update anotasi\n",
    "            for ann in data['annotations']:\n",
    "                if ann['category_id'] not in id_map:\n",
    "                    continue\n",
    "\n",
    "                ann['id'] = len(merged_data['annotations'])\n",
    "                ann['image_id'] = current_image_id_map[ann['image_id']]\n",
    "                ann['category_id'] = id_map[ann['category_id']]\n",
    "                merged_data['annotations'].append(ann)\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "# Jalankan fungsi merge\n",
    "print(\"--- STARTING MERGE PROCESS ---\")\n",
    "merged_coco = merge_coco_datasets_all_splits(datasets_info, MASTER_CATEGORIES, output_path)\n",
    "\n",
    "# Simpan file COCO gabungan sementara\n",
    "merged_json_path = os.path.join(output_path, '_merged_annotations.coco.json')\n",
    "with open(merged_json_path, 'w') as f:\n",
    "    json.dump(merged_coco, f, indent=2)\n",
    "\n",
    "print(\"\\n--- MERGE COMPLETE ---\")\n",
    "print(f\"Total unique images merged: {len(merged_coco['images'])}\")\n",
    "print(f\"Total harmonized annotations merged: {len(merged_coco['annotations'])}\")\n",
    "print(f\"Merged annotations file saved to: {merged_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7341c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# FASE 3: MEMBUAT SPLIT TRAIN/VALID/TEST BARU\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def create_split_files(image_list, source_coco, split_name, output_dir):\n",
    "    # Buat folder untuk split ini, termasuk subfolder 'images'\n",
    "    img_output_path = os.path.join(output_dir, split_name)\n",
    "    os.makedirs(img_output_path, exist_ok=True)\n",
    "    \n",
    "    split_coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": source_coco['categories'],\n",
    "        \"info\": source_coco.get('info', {}),\n",
    "        \"licenses\": source_coco.get('licenses', [])\n",
    "    }\n",
    "    \n",
    "    image_ids_in_split = {img['id'] for img in image_list}\n",
    "    \n",
    "    # 1. Salin gambar dan kumpulkan detail gambar\n",
    "    print(f\"\\nCopying {len(image_list)} images for '{split_name}' split...\")\n",
    "    for img_details in image_list:\n",
    "        split_coco['images'].append(img_details)\n",
    "        \n",
    "        src_path = os.path.join(output_dir, 'images_all', img_details['file_name'])\n",
    "        dst_path = os.path.join(img_output_path, img_details['file_name'])\n",
    "        if os.path.exists(src_path):\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "        else:\n",
    "            print(f\"  [WARNING] Could not find source image to copy: {src_path}\")\n",
    "            \n",
    "    # 2. Saring anotasi berdasarkan ID gambar\n",
    "    split_coco['annotations'] = [ann for ann in source_coco['annotations'] if ann['image_id'] in image_ids_in_split]\n",
    "            \n",
    "    # 3. Simpan file JSON untuk split ini\n",
    "    json_path = os.path.join(output_dir, split_name, '_annotations.coco.json')\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(split_coco, f, indent=2)\n",
    "        \n",
    "    print(f\"Created '{split_name}' split: {len(split_coco['images'])} images, {len(split_coco['annotations'])} annotations.\")\n",
    "    print(f\"Annotations saved to: {json_path}\")\n",
    "\n",
    "# --- Eksekusi Re-split ---\n",
    "print(\"\\n--- STARTING RE-SPLIT PROCESS ---\")\n",
    "all_images = merged_coco['images']\n",
    "\n",
    "# Rasio: 70% Train, 20% Validation, 10% Test\n",
    "train_val_images, test_images = train_test_split(all_images, test_size=0.1, random_state=42)\n",
    "train_images, val_images = train_test_split(train_val_images, test_size=0.22, random_state=42) # 0.22 * 0.9 ‚âà 0.2\n",
    "\n",
    "print(f\"\\nNew dataset sizes: Train={len(train_images)}, Valid={len(val_images)}, Test={len(test_images)}\")\n",
    "\n",
    "# Buat masing-masing dataset split\n",
    "create_split_files(train_images, merged_coco, 'train', output_path)\n",
    "create_split_files(val_images, merged_coco, 'valid', output_path)\n",
    "create_split_files(test_images, merged_coco, 'test', output_path)\n",
    "\n",
    "# Hapus folder & file sementara\n",
    "shutil.rmtree(os.path.join(output_path, 'images_all'))\n",
    "os.remove(os.path.join(output_path, '_merged_annotations.coco.json'))\n",
    "\n",
    "print(\"\\n--- PROCESS FINISHED! ---\")\n",
    "print(f\"Your final, re-splitted dataset is ready in: '{output_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fc363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Setup reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# MASTER CATEGORIES & DATASET MAPPING\n",
    "# -----------------------------------------------------------------------------\n",
    "MASTER_CATEGORIES = [\n",
    "    {\"id\": 0, \"name\": \"Angel/Manfish\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 1, \"name\": \"Betta\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 2, \"name\": \"Goldfish\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 3, \"name\": \"Koi\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 4, \"name\": \"Tetra\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 5, \"name\": \"Botia\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 6, \"name\": \"Molly\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 7, \"name\": \"Guppy\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 8, \"name\": \"Rainbow\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 9, \"name\": \"Komet\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 10, \"name\": \"Lemon Cichlid\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 11, \"name\": \"Jewel Cichlid\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 12, \"name\": \"Polar Blue Parrot\", \"supercategory\": \"fish\"},\n",
    "    {\"id\": 13, \"name\": \"Tiger Barb\", \"supercategory\": \"fish\"}\n",
    "]\n",
    "\n",
    "# Dataset info\n",
    "datasets_info = [\n",
    "    {\n",
    "        \"base_path\": 'Dataset/Ornamental Fish',\n",
    "        \"id_map\": {\n",
    "            1: 4, 2: 5, 3: 3, 4: 3, 5: 3, 6: 3, 7: 3, 8: 3, 9: 3, 10: 3,\n",
    "            11: 3, 12: 3, 13: 3, 14: 3, 15: 3, 16: 3, 17: 2, 18: 2, 19: 2,\n",
    "            20: 2, 21: 2, 22: 9, 23: 10, 24: 0, 25: 8\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"base_path\": 'Dataset/Ornamental Freshwater Fish',\n",
    "        \"id_map\": {\n",
    "            1: 0, 2: 1, 3: 1, 4: 3, 5: 7, 6: 11, 7: 6, 8: 12, 9: 2, 10: 13\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "output_path = './final_combined_dataset/'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FASE 1: MERGE\n",
    "# -----------------------------------------------------------------------------\n",
    "def merge_coco_datasets(datasets, master_categories, output_base_path):\n",
    "    merged_data = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": master_categories,\n",
    "        \"info\": {\"description\": \"Merged ornamental fish dataset\"},\n",
    "        \"licenses\": []\n",
    "    }\n",
    "    image_id_offset = 0\n",
    "    annotation_id_offset = 0\n",
    "\n",
    "    temp_images_path = os.path.join(output_base_path, 'images_all')\n",
    "    os.makedirs(temp_images_path, exist_ok=True)\n",
    "\n",
    "    for dataset_idx, dataset_info in enumerate(datasets):\n",
    "        base_path = dataset_info[\"base_path\"]\n",
    "        id_map = dataset_info[\"id_map\"]\n",
    "        prefix = f\"ds{dataset_idx}_\"\n",
    "\n",
    "        print(f\"\\nMerging from {base_path}\")\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            ann_path = os.path.join(base_path, split, '_annotations.coco.json')\n",
    "            if not os.path.exists(ann_path):\n",
    "                print(f\"  - {split} annotations not found, skipping.\")\n",
    "                continue\n",
    "\n",
    "            with open(ann_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Remap images\n",
    "            img_id_map = {}\n",
    "            for img in data['images']:\n",
    "                new_id = image_id_offset\n",
    "                new_file_name = f\"{prefix}{img['file_name']}\"\n",
    "                img_id_map[img['id']] = new_id\n",
    "                merged_data['images'].append({\n",
    "                    **img,\n",
    "                    'id': new_id,\n",
    "                    'file_name': new_file_name\n",
    "                })\n",
    "                src_img_path = os.path.join(base_path, split, img['file_name'])\n",
    "                if not os.path.exists(src_img_path):\n",
    "                    src_img_path = os.path.join(base_path, split, 'images', img['file_name'])\n",
    "                if os.path.exists(src_img_path):\n",
    "                    shutil.copy2(src_img_path, os.path.join(temp_images_path, new_file_name))\n",
    "                else:\n",
    "                    print(f\"    [WARNING] Missing image: {src_img_path}\")\n",
    "\n",
    "                image_id_offset += 1\n",
    "\n",
    "            # Remap annotations\n",
    "            for ann in data['annotations']:\n",
    "                if ann['category_id'] not in id_map:\n",
    "                    continue\n",
    "                new_ann = ann.copy()\n",
    "                new_ann['id'] = annotation_id_offset\n",
    "                new_ann['image_id'] = img_id_map[ann['image_id']]\n",
    "                new_ann['category_id'] = id_map[ann['category_id']]\n",
    "                merged_data['annotations'].append(new_ann)\n",
    "                annotation_id_offset += 1\n",
    "\n",
    "    return merged_data\n",
    "\n",
    "merged_coco = merge_coco_datasets(datasets_info, MASTER_CATEGORIES, output_path)\n",
    "\n",
    "# Simpan hasil merge sementara\n",
    "merged_json_path = os.path.join(output_path, '_merged_annotations.coco.json')\n",
    "with open(merged_json_path, 'w') as f:\n",
    "    json.dump(merged_coco, f, indent=2)\n",
    "print(f\"\\n‚úÖ Merge complete: {len(merged_coco['images'])} images, {len(merged_coco['annotations'])} annotations.\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FASE 2: SPLIT\n",
    "# -----------------------------------------------------------------------------\n",
    "def create_split(split_name, images, annotations, categories, output_base_path):\n",
    "    img_dir = os.path.join(output_base_path, split_name)\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    # Copy images\n",
    "    for img in images:\n",
    "        src = os.path.join(output_base_path, 'images_all', img['file_name'])\n",
    "        dst = os.path.join(img_dir, img['file_name'])\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "    # Filter annotations\n",
    "    img_ids = {img['id'] for img in images}\n",
    "    filtered_anns = [ann for ann in annotations if ann['image_id'] in img_ids]\n",
    "\n",
    "    # Save coco json\n",
    "    split_coco = {\n",
    "        \"images\": images,\n",
    "        \"annotations\": filtered_anns,\n",
    "        \"categories\": categories,\n",
    "        \"info\": {},\n",
    "        \"licenses\": []\n",
    "    }\n",
    "    with open(os.path.join(img_dir, '_annotations.coco.json'), 'w') as f:\n",
    "        json.dump(split_coco, f, indent=2)\n",
    "    print(f\"{split_name}: {len(images)} images, {len(filtered_anns)} annotations.\")\n",
    "\n",
    "# Split\n",
    "train_val, test = train_test_split(merged_coco['images'], test_size=0.1, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.22, random_state=42)\n",
    "\n",
    "print(\"\\n--- Creating splits ---\")\n",
    "create_split('train', train, merged_coco['annotations'], MASTER_CATEGORIES, output_path)\n",
    "create_split('valid', val, merged_coco['annotations'], MASTER_CATEGORIES, output_path)\n",
    "create_split('test', test, merged_coco['annotations'], MASTER_CATEGORIES, output_path)\n",
    "\n",
    "# Bersihkan sementara\n",
    "shutil.rmtree(os.path.join(output_path, 'images_all'))\n",
    "os.remove(merged_json_path)\n",
    "print(\"\\nüéâ Done! Dataset siap di folder:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e2e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed303a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ikan_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
